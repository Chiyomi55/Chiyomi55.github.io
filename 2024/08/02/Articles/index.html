<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="Hexo Theme Redefine">
    
    <meta name="author" content="Chiyomi">
    <!-- preconnect -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>

    
    <!--- Seo Part-->
    
    <link rel="canonical" href="http://example.com/2024/08/02/articles/"/>
    <meta name="robots" content="index,follow">
    <meta name="googlebot" content="index,follow">
    <meta name="revisit-after" content="1 days">
    
        <meta name="description" content="https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;638884759  Attention is all you need-论文精读前言需要先了解三个模型： RNN即递归神经网络（Recurrent Neural Network），是一类用于处理序列、数据的神经网络。与传统的前馈神经网络（Feedforward Neural Network）不同，RNN具有循环连接的特点，可以记住以前输入">
<meta property="og:type" content="article">
<meta property="og:title" content="Articles">
<meta property="og:url" content="http://example.com/2024/08/02/Articles/index.html">
<meta property="og:site_name" content="Memoryβ">
<meta property="og:description" content="https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;638884759  Attention is all you need-论文精读前言需要先了解三个模型： RNN即递归神经网络（Recurrent Neural Network），是一类用于处理序列、数据的神经网络。与传统的前馈神经网络（Feedforward Neural Network）不同，RNN具有循环连接的特点，可以记住以前输入">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2024/08/02/Articles/1722671865943.png">
<meta property="og:image" content="http://example.com/2024/08/02/Articles/1722674479518.png">
<meta property="og:image" content="http://example.com/2024/08/02/Articles/1722674866926.png">
<meta property="og:image" content="http://example.com/2024/08/02/Articles/1722674876973.png">
<meta property="og:image" content="http://example.com/2024/08/02/Articles/1722674886354.png">
<meta property="og:image" content="http://example.com/2024/08/02/Articles/1722674902311.png">
<meta property="og:image" content="http://example.com/2024/08/02/Articles/1722674912107.png">
<meta property="og:image" content="http://example.com/2024/08/02/Articles/1722674921670.png">
<meta property="og:image" content="http://example.com/Articles/v2-47a8869974704430868ba86814a92aab_1440w.png">
<meta property="og:image" content="http://example.com/2024/08/02/Articles/1722652760327.png">
<meta property="og:image" content="http://example.com/2024/08/02/Articles/1722652966257.png">
<meta property="og:image" content="http://example.com/2024/08/02/Articles/1722653156466.png">
<meta property="og:image" content="http://example.com/2024/08/02/Articles/1722654068244.png">
<meta property="og:image" content="http://example.com/2024/08/02/Articles/1722654264638.png">
<meta property="og:image" content="http://example.com/2024/08/02/Articles/v2-f9f5e7d0f56ab7e47a3e6288cf0872d4_720w.webp">
<meta property="og:image" content="http://example.com/2024/08/02/Articles/v2-57318dd3aff24f4b2d813ec72d3ab60b_720w.webp">
<meta property="article:published_time" content="2024-08-02T12:46:39.000Z">
<meta property="article:modified_time" content="2024-08-03T09:10:54.118Z">
<meta property="article:author" content="Chiyomi">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2024/08/02/Articles/1722671865943.png">
    
    
    <!--- Icon Part-->
    <link rel="icon" type="image/png" href="/images/animal-print1.png" sizes="192x192">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/animal-print1.png">
    <meta name="theme-color" content="#A31F34">
    <link rel="shortcut icon" href="/images/animal-print1.png">
    <!--- Page Info-->
    
    <title>
        
            Articles -
        
        Memoryβ
    </title>

    
<link rel="stylesheet" href="/fonts/Chillax/chillax.css">


    <!--- Inject Part-->
    

    
<link rel="stylesheet" href="/css/style.css">


    
        
<link rel="stylesheet" href="/assets/build/styles.css">

    

    
<link rel="stylesheet" href="/fonts/GeistMono/geist-mono.css">

    
<link rel="stylesheet" href="/fonts/Geist/geist.css">

    <!--- Font Part-->
    
    
    
    

    
        
<script src="/js/libs/anime.min.js"></script>

    

    <script id="hexo-configurations">
    window.config = {"hostname":"example.com","root":"/","language":"en"};
    window.theme = {"articles":{"style":{"font_size":"16px","line_height":1.5,"image_border_radius":"14px","image_alignment":"center","image_caption":false,"link_icon":true,"title_alignment":"left","headings_top_spacing":{"h1":"3.2rem","h2":"2.4rem","h3":"1.9rem","h4":"1.6rem","h5":"1.4rem","h6":"1.3rem"}},"word_count":{"enable":true,"count":true,"min2read":true},"author_label":{"enable":true,"auto":false,"list":[]},"code_block":{"copy":true,"style":"mac","font":{"enable":false,"family":null,"url":null}},"toc":{"enable":true,"max_depth":3,"number":false,"expand":true,"init_open":true},"copyright":{"enable":true,"default":"cc_by_nc_sa"},"lazyload":true,"recommendation":{"enable":false,"title":"推荐阅读","limit":3,"mobile_limit":2,"placeholder":"/images/wallhaven-wqery6-light.webp","skip_dirs":[]}},"colors":{"primary":"#A31F34","secondary":null,"default_mode":"light"},"global":{"fonts":{"chinese":{"enable":false,"family":null,"url":null},"english":{"enable":false,"family":null,"url":null}},"content_max_width":"1000px","sidebar_width":"210px","hover":{"shadow":true,"scale":false},"scroll_progress":{"bar":false,"percentage":true},"website_counter":{"url":"https://cn.vercount.one/js","enable":true,"site_pv":true,"site_uv":true,"post_pv":true},"single_page":true,"preloader":true,"open_graph":true,"google_analytics":{"enable":false,"id":null}},"home_banner":{"enable":true,"style":"fixed","image":{"light":"/images/wallhaven-wqery6-light.webp","dark":"/images/wallhaven-wqery6-dark.webp"},"title":"(⸝⸝>ᴗ(>ᴗ<⸝⸝）","subtitle":{"text":["Per aspera ad astra."],"hitokoto":{"enable":false,"api":"https://v1.hitokoto.cn"},"typing_speed":100,"backing_speed":80,"starting_delay":500,"backing_delay":1500,"loop":true,"smart_backspace":true},"text_color":{"light":"#fff","dark":"#d1d1b6"},"text_style":{"title_size":"2.8rem","subtitle_size":"1.5rem","line_height":1.2},"custom_font":{"enable":false,"family":null,"url":null},"social_links":{"enable":true,"style":"default","links":{"github":null,"instagram":null,"zhihu":null,"twitter":null,"email":null},"qrs":{"weixin":null}}},"plugins":{"feed":{"enable":false},"aplayer":{"enable":false,"type":"fixed","audios":[{"name":null,"artist":null,"url":null,"cover":null,"lrc":null}]},"mermaid":{"enable":false,"version":"9.3.0"}},"version":"2.6.4","navbar":{"auto_hide":false,"color":{"left":"#f78736","right":"#367df7","transparency":35},"width":{"home":"1200px","pages":"1000px"},"links":{"Home":{"path":"/","icon":"fa-regular fa-house"}},"search":{"enable":false,"preload":true}},"page_templates":{"friends_column":2,"tags_style":"blur"},"home":{"sidebar":{"enable":true,"position":"left","first_item":"menu","announcement":null,"show_on_mobile":true,"links":null},"article_date_format":"auto","categories":{"enable":true,"limit":3},"tags":{"enable":true,"limit":3}},"footerStart":"2024/7/19 20:30:14"};
    window.lang_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
    window.data = {"masonry":false};
  </script>
    
    <!--- Fontawesome Part-->
    
<link rel="stylesheet" href="/fontawesome/fontawesome.min.css">

    
<link rel="stylesheet" href="/fontawesome/brands.min.css">

    
<link rel="stylesheet" href="/fontawesome/solid.min.css">

    
<link rel="stylesheet" href="/fontawesome/regular.min.css">

    
    
    
    
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
<div class="progress-bar-container">
    

    
        <span class="pjax-progress-bar"></span>
<!--        <span class="swup-progress-icon">-->
<!--            <i class="fa-solid fa-circle-notch fa-spin"></i>-->
<!--        </span>-->
    
</div>



    <style>
    :root {
        --preloader-background-color: #fff;
        --preloader-text-color: #000;
    }

    @media (prefers-color-scheme: dark) {
        :root {
            --preloader-background-color: #202124;
            --preloader-text-color: #fff;
        }
    }

    @media (prefers-color-scheme: light) {
        :root {
            --preloader-background-color: #fff;
            --preloader-text-color: #000;
        }
    }

    @media (max-width: 600px) {
        .ml13 {
            font-size: 2.6rem !important; /* Adjust this value as needed */
        }
    }

    .preloader {
        display: flex;
        flex-direction: column;
        gap: 1rem; /* Tailwind 'gap-4' is 1rem */
        align-items: center;
        justify-content: center;
        position: fixed;
        padding: 12px;
        top: 0;
        right: 0;
        bottom: 0;
        left: 0;
        width: 100vw;
        height: 100vh; /* 'h-screen' is 100% of the viewport height */
        background-color: var(--preloader-background-color);
        z-index: 1100; /* 'z-[1100]' sets the z-index */
        transition: opacity 0.2s ease-in-out;
    }

    .ml13 {
        font-size: 3.2rem;
        /* text-transform: uppercase; */
        color: var(--preloader-text-color);
        letter-spacing: -1px;
        font-weight: 500;
        font-family: 'Chillax-Variable', sans-serif;
        text-align: center;
    }

    .ml13 .word {
        display: inline-flex;
        flex-wrap: wrap;
        white-space: nowrap;
    }

    .ml13 .letter {
        display: inline-block;
        line-height: 1em;
    }
</style>

<div class="preloader">
    <h2 class="ml13">
        Memoryβ
    </h2>
    <script>
        var textWrapper = document.querySelector('.ml13');
        // Split text into words
        var words = textWrapper.textContent.trim().split(' ');

        // Clear the existing content
        textWrapper.innerHTML = '';

        // Wrap each word and its letters in spans
        words.forEach(function(word) {
            var wordSpan = document.createElement('span');
            wordSpan.classList.add('word');
            wordSpan.innerHTML = word.replace(/\S/g, "<span class='letter'>$&</span>");
            textWrapper.appendChild(wordSpan);
            textWrapper.appendChild(document.createTextNode(' ')); // Add space between words
        });

        var animation = anime.timeline({loop: true})
            .add({
                targets: '.ml13 .letter',
                translateY: [40,0],
                translateZ: 0,
                opacity: [0,1],
                filter: ['blur(5px)', 'blur(0px)'], // Starting from blurred to unblurred
                easing: "easeOutExpo",
                duration: 1400,
                delay: (el, i) => 300 + 30 * i,
            }).add({
                targets: '.ml13 .letter',
                translateY: [0,-40],
                opacity: [1,0],
                filter: ['blur(0px)', 'blur(5px)'], // Ending from unblurred to blurred
                easing: "easeInExpo",
                duration: 1200,
                delay: (el, i) => 100 + 30 * i,
                complete: function() {
                    hidePreloader(); // Call hidePreloader after the animation completes
                }
            });

        let themeStatus = JSON.parse(localStorage.getItem('REDEFINE-THEME-STATUS'))?.isDark;

        // If the theme status is not found in local storage, check the preferred color scheme
        if (themeStatus === undefined || themeStatus === null) {
            if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
                themeStatus = 'dark';
            } else {
                themeStatus = 'light';
            }
        }

        // Now you can use the themeStatus variable in your code
        if (themeStatus) {
            document.documentElement.style.setProperty('--preloader-background-color', '#202124');
            document.documentElement.style.setProperty('--preloader-text-color', '#fff');
        } else {
            document.documentElement.style.setProperty('--preloader-background-color', '#fff');
            document.documentElement.style.setProperty('--preloader-text-color', '#000');
        }

        window.addEventListener('load', function () {
            setTimeout(hidePreloader, 5000); // Call hidePreloader after 5000 milliseconds if not already called by animation
        });

        function hidePreloader() {
            var preloader = document.querySelector('.preloader');
            preloader.style.opacity = '0';
            setTimeout(function () {
                preloader.style.display = 'none';
            }, 200);
        }
    </script>
</div>

<main class="page-container" id="swup">

    

    <div class="main-content-container">


        <div class="main-content-header">
            <header class="navbar-container px-6 md:px-12">

    <div class="navbar-content ">
        <div class="left">
            
            <a class="logo-title" href="/">
                
                Memoryβ
                
            </a>
        </div>

        <div class="right">
            <!-- PC -->
            <div class="desktop">
                <ul class="navbar-list">
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class=""
                                   href="/"
                                        >
                                    <i class="fa-regular fa-house fa-fw"></i>
                                    HOME
                                    
                                </a>

                                <!-- Submenu -->
                                
                            </li>
                    
                    
                </ul>
            </div>
            <!-- Mobile -->
            <div class="mobile">
                
                <div class="icon-item navbar-bar">
                    <div class="navbar-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <!-- Mobile sheet -->
    <div class="navbar-drawer h-screen w-full absolute top-0 left-0 bg-background-color flex flex-col justify-between">
        <ul class="drawer-navbar-list flex flex-col px-4 justify-center items-start">
            
                
                    

                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                           href="/"
                        >
                            <span>
                                HOME
                            </span>
                            
                                <i class="fa-regular fa-house fa-sm fa-fw"></i>
                            
                        </a>
                        

                        
                    </li>
            

            
            
        </ul>

        <div class="statistics flex justify-around my-2.5">
    <a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/tags">
        <div class="number text-2xl sm:text-xl text-second-text-color font-semibold">0</div>
        <div class="label text-third-text-color text-sm">Tags</div>
    </a>
    <a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/categories">
        <div class="number text-2xl sm:text-xl text-second-text-color font-semibold">0</div>
        <div class="label text-third-text-color text-sm">Categories</div>
    </a>
    <a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/archives">
        <div class="number text-2xl sm:text-xl text-second-text-color font-semibold">2</div>
        <div class="label text-third-text-color text-sm">Posts</div>
    </a>
</div>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="main-content-body">

            

            <div class="main-content">

                
                    <div class="post-page-container flex relative justify-between box-border w-full h-full">
    <div class="article-content-container">

        <div class="article-title relative w-full">
            
                <div class="w-full flex items-center pt-6 justify-start">
                    <h1 class="article-title-regular text-second-text-color tracking-tight text-4xl md:text-6xl font-semibold px-2 sm:px-6 md:px-8 py-3">Articles</h1>
                </div>
            
            </div>

        
            <div class="article-header flex flex-row gap-2 items-center px-2 sm:px-6 md:px-8">
                <div class="avatar w-[46px] h-[46px] flex-shrink-0 rounded-medium border border-border-color p-[1px]">
                    <img src="/images/cat.jpg">
                </div>
                <div class="info flex flex-col justify-between">
                    <div class="author flex items-center">
                        <span class="name text-default-text-color text-lg font-semibold">Chiyomi</span>
                        
                            <span class="author-label ml-1.5 text-xs px-2 py-0.5 rounded-small text-third-text-color border border-shadow-color-1">Lv1</span>
                        
                    </div>
                    <div class="meta-info">
                        <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fa-regular fa-pen-fancy"></i>&nbsp;
        <span class="desktop">2024-08-02 20:46:39</span>
        <span class="mobile">2024-08-02 20:46:39</span>
        <span class="hover-info">Created</span>
    </span>
    
        <span class="article-date article-meta-item">
            <i class="fa-regular fa-wrench"></i>&nbsp;
            <span class="desktop">2024-08-03 17:10:54</span>
            <span class="mobile">2024-08-03 17:10:54</span>
            <span class="hover-info">Updated</span>
        </span>
    

    
    

    
    
    
    
        <span class="article-pv article-meta-item">
            <i class="fa-regular fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

                    </div>
                </div>
            </div>
        

        


        <div class="article-content markdown-body px-2 sm:px-6 md:px-8 pb-8">
            <p><a class="link"   target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/638884759" >https://zhuanlan.zhihu.com/p/638884759 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h1 id="Attention-is-all-you-need-论文精读"><a href="#Attention-is-all-you-need-论文精读" class="headerlink" title="Attention is all you need-论文精读"></a>Attention is all you need-论文精读</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>需要先了解三个模型：</p>
<h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a><strong>RNN</strong></h3><p>即<strong>递归神经网络</strong>（Recurrent Neural Network），是一类用于处理序列、数据的神经网络。与传统的前馈神经网络（Feedforward Neural Network）不同，RNN具有循环连接的特点，可以记住以前输入的信息。这使得它特别适合处理时间序列数据、自然语言处理（NLP）等任务。</p>
<p>一、RNN的主要特点包括：</p>
<ol>
<li><strong>序列处理</strong>：RNN能够处理任意长度的输入序列，因为它在每一步都接收当前输入和之前的隐藏状态（即记忆）。</li>
<li><strong>循环连接</strong>：在RNN中，隐藏层的输出不仅传递给下一层，还会传递回自身。这种循环连接使得网络可以保留以前的信息并利用这些信息进行当前的预测或决策。</li>
<li><strong>共享参数</strong>：RNN在每个时间步共享相同的参数，这意味着它在处理不同时间步的数据时应用相同的权重和偏置，从而减少了参数数量和训练复杂度。</li>
</ol>
<p>二、RNN的结构</p>
<p>一个典型的RNN由一个输入层、一个或多个隐藏层和一个输出层组成。隐藏层具有循环连接，用于保留过去的信息。以下是RNN的基本数学公式：</p>
<ul>
<li><p><strong>隐藏状态更新</strong>：<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2024/08/02/Articles/1722671865943.png"
                      class="" width="1722671865943"
                ></p>
</li>
<li><p>其中，ht是当前的隐藏状态，ht−1 是前一时刻的隐藏状态，xt 是当前的输入，Wh和 Wx 分别是权重矩阵，b 是偏置，σ 是激活函数（例如tanh或ReLU）。</p>
</li>
<li><p><strong>输出计算</strong>：<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2024/08/02/Articles/1722674479518.png"
                      class="" width="1722674479518"
                ></p>
</li>
<li><p>其中，yt是当前的输出，Wy 是输出权重矩阵，by 是输出偏置。</p>
</li>
</ul>
<p>尽管RNN在序列处理方面表现出色，但它们也存在一些缺点，如梯度消失和梯度爆炸问题，这在处理长序列时尤为突出。为了解决这些问题，研究人员提出了改进的RNN结构，如长短期记忆网络（LSTM）和门控循环单元（GRU）。</p>
<h3 id="LSTM："><a href="#LSTM：" class="headerlink" title="LSTM："></a>LSTM：</h3><p>长短期记忆网络（LSTM，Long Short-Term Memory）是一种特殊的递归神经网络（RNN）架构，专门设计用来解决标准RNN中常见的梯度消失和梯度爆炸问题，使得LSTM在处理长时间序列数据时表现出色。</p>
<p>一、LSTM的结构</p>
<p>LSTM引入了三个门（门控机制）：输入门、遗忘门和输出门，通过这些门控机制来控制信息的流动。LSTM单元的结构比标准RNN单元更复杂，但这些门控机制使得LSTM能够有效地记住和遗忘信息，从而更好地处理长序列依赖。</p>
<p><strong>主要组件</strong></p>
<ol>
<li><strong>遗忘门（Forget Gate）</strong>：决定哪些信息需要从细胞状态中遗忘。</li>
<li><strong>输入门（Input Gate）</strong>：决定哪些新信息需要被加入到细胞状态中。</li>
<li><strong>细胞状态（Cell State）</strong>：传递长时记忆信息的路径。</li>
<li><strong>输出门（Output Gate）</strong>：决定细胞状态的哪些部分将作为输出。</li>
</ol>
<p><strong>数学公式</strong></p>
<ul>
<li><strong>遗忘门</strong>：<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2024/08/02/Articles/1722674866926.png"
                      class="" width="1722674866926"
                ></li>
<li><strong>输入门</strong>：<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2024/08/02/Articles/1722674876973.png"
                      class="" width="1722674876973"
                ></li>
<li><strong>新信息</strong>：<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2024/08/02/Articles/1722674886354.png"
                      class="" width="1722674886354"
                ></li>
<li><strong>细胞状态更新</strong>：<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2024/08/02/Articles/1722674902311.png"
                      class="" width="1722674902311"
                ></li>
<li><strong>输出门</strong>：<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2024/08/02/Articles/1722674912107.png"
                      class="" width="1722674912107"
                ></li>
<li><strong>隐藏状态更新</strong>：<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2024/08/02/Articles/1722674921670.png"
                      class="" width="1722674921670"
                ></li>
</ul>
<p>其中，σ 表示sigmoid函数，tanh表示tanh函数，[ht−1,xt] 表示前一个隐藏状态和当前输入的连接。</p>
<p>二、LSTM的优势</p>
<ol>
<li><strong>解决长依赖问题</strong>：LSTM的门控机制使得它在处理长序列数据时能够有效避免梯度消失和梯度爆炸问题。</li>
<li><strong>记住长期信息</strong>：通过细胞状态，LSTM能够记住长期信息并选择性地遗忘无关信息。</li>
<li><strong>灵活性</strong>：LSTM在自然语言处理、时间序列预测、视频处理等多个领域表现出色。</li>
</ol>
<h3 id="CNN："><a href="#CNN：" class="headerlink" title="CNN："></a>CNN：</h3><p>卷积神经网络（CNN，Convolutional Neural Network）是一类专门用于处理具有网格结构数据（如图像）的深度学习模型。CNN特别擅长于图像和视频的识别和分类任务，因为它们能够捕捉数据中的空间和时间依赖性。</p>
<p><strong>一、CNN的结构</strong></p>
<p>CNN由多层堆叠而成，这些层通常包括卷积层、池化层（也称为下采样层）和全连接层。以下是这些层的详细介绍：</p>
<ol>
<li><p><strong>卷积层（Convolutional Layer）</strong>：卷积层是CNN的核心，它通过滤波器（也称为卷积核）在输入数据上滑动，提取局部特征。卷积操作可以捕捉到输入数据的空间特征。</p>
<p>数学表达式： (X∗W)[i,j]&#x3D;∑m∑nX[i+m,j+n]⋅W[m,n](X * W)[i, j] &#x3D; \sum_m \sum_n X[i+m, j+n] \cdot W<a href="X%E2%88%97W">m, n</a>[i,j]&#x3D;∑m∑nX[i+m,j+n]⋅W[m,n] 其中，XXX是输入数据，WWW是卷积核，∗*∗表示卷积操作。</p>
</li>
<li><p><strong>池化层（Pooling Layer）</strong>：池化层通过下采样操作减少数据的尺寸，同时保留重要特征。常见的池化操作有最大池化（Max Pooling）和平均池化（Average Pooling）。</p>
<ul>
<li>最大池化（Max Pooling）：取池化窗口内的最大值。</li>
<li>平均池化（Average Pooling）：取池化窗口内的平均值。</li>
</ul>
</li>
<li><p><strong>全连接层（Fully Connected Layer）</strong>：全连接层与传统的神经网络层类似，将前一层的输出展开成一维向量，然后进行线性变换和非线性激活，用于最终的分类或回归任务。</p>
</li>
</ol>
<p><strong>二、CNN的特点</strong></p>
<ul>
<li><strong>局部感受野</strong>：卷积操作通过局部连接的滤波器来提取输入数据的局部特征。</li>
<li><strong>参数共享</strong>：同一卷积核在整个输入数据上滑动，减少了模型的参数数量。</li>
<li><strong>平移不变性</strong>：卷积操作具有平移不变性，可以在不同位置检测到相同的特征。</li>
</ul>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>主流的序列转换模型主要基于复杂的循环神经网络或者卷积神经网络, 这些结构往往包含编码器和解码器架构. 表现最好的模型使用<strong>注意力机制将编码器和加码器连接起来</strong>. 作者提出一种新的简单的网络, transformer, 仅仅使用注意力机制, <strong>完全抛弃了循环结构和卷积结构</strong>. 在两个机器翻译任务上的实验表明, 作者提出的这些网络质量更好, 同时并行化程度更高, 训练时间更短. 作者提出的模型在WMT 2014 English-to-German <strong>翻译任务</strong>中取得了28.4BLEU, 超过了现有的最佳结果2个点. 在这项任务中, 作者的模型建立了一个新的单模型, 仅仅用8GPU训练3.5天解达到了最高的41.8的BLEU分数, 是当时最优模型的很小一部分花费. 作者证明了transformer能够在其他任务上同样取得很好的成果, 例如在大量和有限数据上训练的English constituency parsing（英语句法分析）任务上都获得了成功. </p>
<h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1.介绍"></a>1.介绍</h2><p><strong>循环神经网络(RNN)</strong>, <strong>长短期记忆网络(LSTM)<strong>和</strong>门控(RNN)</strong>, 已经在序列模型和转换问题比如语言模型和机器翻译上建立了最前沿有效的方法地位。很多努力在不断地推动循环神经网络语言模型和编码-解码结构的边界前进。</p>
<p>循环模型通常是沿着输入和输出序列符号进行计算的. 将位置映射到计算的时间步的过程中, 循环模型产生了一系列的中间隐藏过程h_t, h_t是前面状态h_{t-1}和当前输入t的函数. <strong>这种序列固有的性质在训练时限制了并行化, 并行化对于长序列是很重要的, 这种限制是因为内存局限了不同序列之间的批处理.</strong> 最近的工作在计算效率上取得了显著的提升, 采用的方法为factorization\ tricks和conditional \ computation, 并且在conditional \ computation,中也获得了模型效果的提升. 但序列计算的根本限制仍然存在, 没有被解决.</p>
<p>注意力机制已经在很多任务的复杂序列模型和转换模型中成为重要的一部分, 能够让模型在输入序列和输出序列中建立各个部分的依赖, 而忽略位置距离影响. 在很多情况下, 然而这种注意力机制往往是和一个循环网络一起使用.</p>
<p>在这项工作中, 作者提出了transformer, 是一种完全避免了recurrence(循环)而是仅仅依赖注意力机制, 在输入和输出之间建立全局依赖的模型框架. Transformer 允许<strong>高度并行化</strong>, 并在翻译质量通过仅仅在8个P100 GPU上训练12个小时就达到了最高水平.</p>
<h2 id="2-背景"><a href="#2-背景" class="headerlink" title="2.背景"></a>2.背景</h2><p>为了减少序列计算, Extended\ Neural\ GPU ByteNet ConvS2S 使用了卷积神经网络作为基本的模块,对所有的输入输出位置并行化计算中间表示(hidden representations). 在这些模型中, 用于连接两个输出输出的任意位置的操作随着距离的增加而增加, 在ConvS2S中线性增长, 在ByteNet中成log函数增长. 这使得计算两个相距远的位置依赖的难度上升. 在transformer中, 这被减少为常数操作数中, 尽管由于使用平均注意力加权位置带来了效率下降的代价, 我们使用多头注意力机制来抵消.(??)</p>
<p><strong>自注意力机制</strong>, 有时被叫做intra-attention是与单个句子不同位置相关的注意力机制, 目的在于计算一个句子的表示. 自注意力机制已经被成功应用在很多任务中, 比如阅读理解, 概要生成, 学习与任务相关的句子表示。</p>
<p><strong>端到端记忆网络</strong>是主要基于循环注意力机制而不是序列依赖循环, 并且在简单语言的问答和语言模型任务上取得了很好的效果。</p>
<p>在作者调研, <strong>transformer是第一个仅仅依靠自注意力机制来计算输入输出表示的转换模型</strong>, 不使用序列对齐的RNN或者卷积神经网络. 在接下来的战绩, 详细介绍transformer中的自主力机制和相比于其他模型的优点。</p>
<h2 id="3-模型架构"><a href="#3-模型架构" class="headerlink" title="3.模型架构"></a>3.模型架构</h2><p>大多数前沿的神经序列推导模型用encoder-decoder架构.这里, encoder将输入序列的符号表示(x_1,x_2,…x_n) 映射到一个连续的序列表示z&#x3D;(z_1,z_2,…z_n). 给出z, 解码器生成逐个元素的生成输出序列y_1, y_2,…y_m. 在每一步中, 模型都是自回归的, 使用前面已经生成的符号作为额外输出来产生下一个元素.</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/Articles/v2-47a8869974704430868ba86814a92aab_1440w.png"
                      alt="Attention Is All You Need[论文精读-翻译+精细解读]"
                ></p>
<p>transformer遵循这种编码器解码器架构, 使用<strong>堆叠自注意力机制和point-wise,</strong> 在encoder和decoder中都使用全连接层.</p>
<h3 id="3-1-Encode-and-Decoder-Stacks"><a href="#3-1-Encode-and-Decoder-Stacks" class="headerlink" title="3.1 Encode and Decoder Stacks"></a>3.1 Encode and Decoder Stacks</h3><p><strong>Encoder:</strong> encoder由N&#x3D;6个独立的层堆叠而成.每一个层大都有两个子层. </p>
<p>第一子层为多头注意力机制, 第二个子层为简单的position-wise的全连接网络. </p>
<p>作者在每个子层中使用残差连接和layer normalization.也就是对于每个子层输出为LayerNorm(x+Sublayer(x)) , Sublayer(x)是每个子层使用的函数.</p>
<p> 为了利用残差连接, 模型中的所有子层, 包括embedding层的输出维度为512.</p>
<hr>
<p><strong>Decoder:</strong> Decoder同样由N&#x3D;6个独立层堆叠而成. </p>
<p>相比于encoder的两个子层增加了一个子层, 在encoder的输出上使用多头注意力机制. </p>
<p>和encoder类似的, 作者在每个子层中使用残差连接并使用layer norm. </p>
<p>作者修改了decoder中的self-attention机制来防止每个位置参考后面的位置. </p>
<p>这种masking(遮掩), 是通过现实情况每个位置只由前面的信息得到推测来的, 确保每个位置的预测值只依赖该位置前面的信息得到.</p>
<h3 id="3-2-Attention"><a href="#3-2-Attention" class="headerlink" title="3.2 Attention"></a>3.2 Attention</h3><p> &#x3D;&#x3D;<strong>注意力机制</strong>&#x3D;&#x3D;可以被描述为匹配查询query和一系列的键值对key-value到输出, 所有的query&#x2F;keys&#x2F;values为向量. 输出是values的加权平均和,。</p>
<p>权值weight是通过与query和相应的键值相关的compatibility函数得到的. </p>
<h4 id="3-2-1-Scaled-Dot-Product-Attention"><a href="#3-2-1-Scaled-Dot-Product-Attention" class="headerlink" title="3.2.1 Scaled Dot-Product Attention"></a>3.2.1 Scaled Dot-Product Attention</h4><p><strong>&#x3D;&#x3D;缩放点积Attention&#x3D;&#x3D;</strong></p>
<p>输入包括维度为d_k的查询queries和keys, 以及维度为d_v的values. 作者计算一个query和所有keys的点积, 然后除以 dk, 并使用softmax函数来计算每一个value的权重.</p>
<p>在实践中, attention函数在大量queries上同时操作, 将queries构成矩阵Q,.keys和values也被打包成K,V.计算输出矩阵为</p>
<p>Attention(Q,K,V)&#x3D;softmax(QKTdk)V</p>
<p>最经常使用的注意力函数包括加性注意力机制和点积(乘性)注意力函数. 点乘注意力机制和出去scale的这部分模型完全相同.嘉兴注意力计算compatibility function通过一个含有一个隐藏层的前向网络得到. 尽管两个的理论复杂度近似, 实际中点乘注意力更快并且更节省空间, 因为点乘注意力可以使用高度优化的矩阵乘法代码来完成.</p>
<p>实际中发现, 对于较小的值d_k 两种机制产生的效果类似, 加性注意力机制在d_k值较大时取得比点乘注意力更好的效果. 我们猜想是由于 点乘导致数据增加更大, 使得softmax函数到达梯度很小的区域. 为了抵消这种影响, 我们将点乘使用 dk 放缩.(??)</p>
<blockquote>
<p><strong>为什么在进行softmax之前需要对attention进行scaled（为什么除以dk的平方根），并使用公式推导进行讲解</strong><br>假设Q,K的均值为0, 方差为1. 矩阵乘积有均值为0, 方差为dk, 使用dk的平方根被用于缩放, 因为Q&#x2F;K的矩阵乘积本应该为均值为0, 方差为1, 这样可以获得更平缓的softmax.<br>当维度增大时, 会导致数值增大, 就会导致softmax**<a class="link"   href="https://link.zhihu.com/?target=https://www.google.com.hk/search?q=Softmax+gradient+vanish+attention&newwindow=1&rlz=1C1CHWL_zh-CNCN943CN943&sxsrf=ALiCzsaAGPr9qE7AsR1srN5wBWweCKSW2Q:1657808730844&ei=WifQYsKQM9eM4-EPgPuzUA&ved=0ahUKEwjCpYfkyvj4AhVXxjgGHYD9DAoQ4dUDCA4&uact=5&oq=Softmax+gradient+vanish+attention&gs_lcp=Cgdnd3Mtd2l6EAMyBQghEKABMgUIIRCgAToECAAQRzoECAAQHjoHCCEQChCgAUoECEEYAEoECEYYAFCcBFiNGWD6HGgAcAJ4AIABpwKIAbkOkgEFMC45LjGYAQCgAQHIAQrAAQE&sclient=gws-wiz" >梯度消失 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>**. 通过scale\sqrt{d_k}, 方差会scale dk.(可推导)<br>ref: <strong><a class="link"   target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/54356280" >解释 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></strong></p>
</blockquote>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2024/08/02/Articles/1722652760327.png"
                       alt="1722652760327" style="zoom:50%;" 
                >

<h4 id="3-2-2-Multi-Head-Attention"><a href="#3-2-2-Multi-Head-Attention" class="headerlink" title="3.2.2 Multi-Head Attention"></a>3.2.2 Multi-Head Attention</h4><p>不是使用单个注意力函数维度只有模型原本的维度d_{model} –keys,values和queries的维度, 我们发现将queries, keys和values线性映射h次到不同的维度d_k,d_k,d_v, 效果更好.在每次映射到不同queries,keys, values的这些版本中, 作者并行化运行注意力机制, 得到d_v维度的输出. 这些输出被连接起来, 然后再被线性映射到一个维度作为最后的输出.</p>
<p>**&#x3D;&#x3D;多头注意力机制&#x3D;&#x3D;**允许模型联合关注到在不同表示空间中的不同位置.在用单头注意力机制的情况下, 相当于把这些做了一个平均.</p>
<p>在作者的工作中, 取h&#x3D;8并行化注意力层或者头.将每个按照h进行维度缩减, 然后最后再联合起来, 最终计算量和原来相似.</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2024/08/02/Articles/1722652966257.png"
                      class="" width="1722652966257"
                >

<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2024/08/02/Articles/1722653156466.png"
                      class="" width="1722653156466"
                > 

<hr>
<p>为什么要将原始输入都乘上一个不同的W?<br>为了将原始的embedding映射到不同的空间, 更加从不同的层面挖掘信息.另外, 如果都是一样的, 那么Q*V,将得到一个对称矩阵. 因为是同样的矩阵, 投影到了同样的空间, 泛化能力差, 对V进行提纯时效果不好.(为了打破对称).<br>ref: <strong><a class="link"   target="_blank" rel="noopener" href="https://www.zhihu.com/question/319339652" >解释 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></strong><br>为什么使用多头? 为了让模型能关注到不同位置在不同表示空间中的信息. </p>
<h4 id="3-2-3-Applications-of-Attention-in-our-Model"><a href="#3-2-3-Applications-of-Attention-in-our-Model" class="headerlink" title="3.2.3 Applications of Attention in our Model"></a>3.2.3 Applications of Attention in our Model</h4><p>&#x3D;&#x3D;<strong>模型中attention机制的应用</strong>&#x3D;&#x3D;; transformer用三种不同的方式使用了多头注意力机制.</p>
<ul>
<li>在encoder-decoder注意力层, queries来自decoder的前一层, keys和values来自encoder.这decoder的每一个位置都能与输入的所有位置相联系. 这种是模型了经典的encoder-decoder注意力机制在序列模型中的结构.</li>
<li>在encoder中包含自注意层.自注意力层中的keys, values, queries都来自前一个层的输出. encoder的每一个位置都能参照联系前一层的所有位置(attend to all positions in the previous layer).</li>
<li>类似的, 自注意力机制让decoder能够在每个位置与到这个位置为止(包含这个位置)的序列相联系. 在解码器中我们需要防止信息向左流动来保护自回归性质.作者在softmax层前面输入的所有的scaled dot-product attention模块中通过mask掉(设为负无穷)所有的非法部分.</li>
</ul>
<blockquote>
<p>autoregressive <a class="link"   href="https://link.zhihu.com/?target=https://slyne.github.io/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2020/03/08/DGM-autoregressive-model/" >property <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>: 定义了一个顺序(ordering)，第 ii 个随机变量按照这个选定的顺序依赖在它量前面的所有随机变量 x1,x2,…,xi−1x1,x2,…,xi−1。</p>
</blockquote>
<h3 id="3-3-Position-wise-Feed-Forward-Networks"><a href="#3-3-Position-wise-Feed-Forward-Networks" class="headerlink" title="3.3 Position-wise Feed-Forward Networks"></a><strong>3.3 Position-wise Feed-Forward Networks</strong></h3><p>在encoder和decoder中每层除了注意力子层外, 还有**&#x3D;&#x3D;完全连接的前馈网络&#x3D;&#x3D;<strong>, 在每个位置独立使用. 这包含</strong><em>两个线性转换和一个ReLU激活函数</em>**在两者之间.</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2024/08/02/Articles/1722654068244.png"
                      class="" width="1722654068244"
                >

<p>两个线性转换作用于相同的位置, 但是在他们用的参数不同. 另一种描述方式时比作两个kernel size&#x3D;1的卷积核. 输入输出的维度为512, 中间维度为2048.</p>
<h3 id="3-4-Embeddings-and-Softmax"><a href="#3-4-Embeddings-and-Softmax" class="headerlink" title="3.4 Embeddings and Softmax"></a><strong>3.4 Embeddings and Softmax</strong></h3><p>和其他的序列推导模型类似, 作者使用学习到的embedding来把输入符号和输出符号转化为维度为d_model的向量. 作者同样使用通用的Linear层和softmax层来讲decoder输出转化为预测的下一个token的概率. 在作者的模型中, 共享输入和输出(??)的权重矩阵和pre-softmax linear transformation, 在编码层, 将权重乘\sqrt{d_{model}}</p>
<h3 id="3-5-Positional-Encoding"><a href="#3-5-Positional-Encoding" class="headerlink" title="3.5 Positional Encoding"></a><strong>3.5 Positional Encoding</strong></h3><p>因为我们的模型没有recurrence和convolution, 为了使模型利用序列信息, 作者添加了一些token位置相对或者绝对信息. 最终添加了**&#x3D;&#x3D;positional encoding位置编码&#x3D;&#x3D;**在输入embedding中, 在encoder和decoder中都添加. 位置编码和embedding表示有相同的维度, 一遍两者能够加和. positional encoding有很多选择, 在这项工作中, 使用不同频率的sine和cosine函数.</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2024/08/02/Articles/1722654264638.png"
                      class="" width="1722654264638"
                >

<p>pos为位置, i为维度.在每个位置的每个维度都对应到一个正弦值.波长形成了一个从2π到10000·2π的几何级数.作者使用这个函数是应为猜测它能够使模型表示位置相对信息, 因为对于任意的PE_{pos+k}可以被表示成PE_{pos}的线性函数.</p>
<p>作者同样尝试了两种版本的学习训练的位置embedding并几乎产生了类似的效果. 作者最终选择sinusoidal版本是因为这个可以使模型能够得出长度大于训练时长度的序列的编码.</p>
<h2 id="4-Why-Self-Attention"><a href="#4-Why-Self-Attention" class="headerlink" title="4 Why Self-Attention"></a><strong>4 Why Self-Attention</strong></h2><p>Self-attention layers和recurrent 和 convolutional layer 通常被用作匹配一个变量长度的序列到另一个等长的序列表示, 比如用作序列预测任务的隐藏层. 作者从不同的方面比较了这三种模型. 从以下三个方面探究 self-attention的作用:</p>
<ul>
<li>在每一层中总的计算复杂度. (Complexity per Layer)</li>
<li>在最小操作数下, 计算可以并行数量, 通常用序列操作所需的最少数量来表示.(Sequential Operations)</li>
<li>在网络长距离依赖中的路径长度. 在很多 sequence transduction 任务中, 学习长距离依赖是一项重要的任务. 影响这项能力的重要因素是网络中的前向传播的长度和反向传播信号的传播.在输入和输出序列中任意位置的组合距离越短, 学习长距离依赖更容易. 因此作者比较了三种网络的在输入和输出位置组合的最大路径长度.(Maximum Path Length)</li>
</ul>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2024/08/02/Articles/v2-f9f5e7d0f56ab7e47a3e6288cf0872d4_720w.webp"
                      class="" title="img"
                >

<p>在表中指出, 自注意力层中联系计算所有位置只需要常数复杂度的操作, 而recurrent需要O(n)次操作. <strong>关于计算复杂度上, self-attention比recurrent 层更快</strong>, 因为序列长度n比表示向量维度d更小, 比如在一些句子表上上最新的机器翻译模型(word-piece, byte-pair)表示中n比d小. 为了提高在一些长句子中的计算表现, 自主力机制可以被限制只考虑输出位置的周围相邻size&#x3D;r的输入序列. 这将提升最大长度距离O(n&#x2F;r).</p>
<blockquote>
<p>缺点: 这里从复杂度来看, transformers的复杂度是n^2, 不适合长句子, 在训练时使用的句子长度为512;<br>计算相似度是计算两两之间的相似度, 对于某些任务是没有必要的,而且计算资源消耗大.<br>对于每一层都是两两计算相似度, 这种可视化出来是否还具有可解释性? 只有比较低的层表示两两之间的相似度, 当层数较高时, 相似度的可解释可视化出来就没有很强了.</p>
</blockquote>
<p>另外自注意力机制也能产生更多可解释性的模型. 作者研究了模型中的注意力分布并附在附录中. 不仅在每个注意力头中学到了应对不同的任务, 并且很多头表现出与句法和句子情感结构相关的特性.</p>
<h2 id="5-Training"><a href="#5-Training" class="headerlink" title="5 Training"></a><strong>5 Training</strong></h2><p>这部分说明了作者模型的训练规则.</p>
<h3 id="5-1-Training-Data-and-Batching"><a href="#5-1-Training-Data-and-Batching" class="headerlink" title="5.1 Training Data and Batching"></a><strong>5.1 Training Data and Batching</strong></h3><p>使用standard WMT 2014 English-German dataset训练, 包含4.5million 句子对. 句子被 通过byte-pair编码, 由37000个tokens字典组成. 对于英法翻译任务, 使用WMT 2014 English-French dataset数据集, 包含36M句子, 并将tokens划分为32000个单词. 长度相似的句子对被划分为一个batch. 每个训练batch包含句子对集合包含约25000个原tokens和25000个目标语言tokens.</p>
<p><strong>5.2 Hardware and Schedule</strong></p>
<p>在8 P100GPU训练.使用罗列出的参数, 每个step要用0.4s.训练了base model用来100,000个step或者12h.对于big model, 每个step用时1s, 总计训练300,000steps(3.5days).</p>
<p><strong>5.3 Optimizer</strong></p>
<p>使用Adam optimizer.学习率在训练过程中根据公式变化.</p>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/2024/08/02/Articles/v2-57318dd3aff24f4b2d813ec72d3ab60b_720w.webp"
                      class="" title="img"
                >



<p><strong>5.4 Regularization</strong></p>
<p>主要目的在于防止过拟合. 使用三种类型的正则化.</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a><strong>参考</strong></h2><ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/54743941" >张俊林CNN&#x2F;RNN&#x2F;Transformer比较 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a><br>参考文章引用: Transformer的可以参考以下三篇文章：一个是Jay Alammar可视化地介绍Transformer的博客文章<a class="link"   href="https://link.zhihu.com/?target=https://jalammar.github.io/illustrated-transformer/%22+%5Ct+%22_blank" >The Illustrated Transformer <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> ，非常容易理解整个机制，建议先从这篇看起， <a class="link"   target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/54356280" >这是中文翻译版本 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>；第二篇是 Calvo的博客：<a class="link"   href="https://link.zhihu.com/?target=https://medium.com/dissecting-bert/dissecting-bert-part-1-d3c3d495cdb3" >Dissecting BERT Part 1: The Encoder <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> ，尽管说是解析Bert，但是因为Bert的Encoder就是Transformer，所以其实它是在解析Transformer，里面举的例子很好；再然后可以进阶一下，参考哈佛大学NLP研究组写的“<a class="link"   href="https://link.zhihu.com/?target=http://nlp.seas.harvard.edu/2018/04/03/attention.html" >The Annotated Transformer. <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> ”，代码原理双管齐下，讲得也很清楚。</li>
<li><a class="link"   href="https://link.zhihu.com/?target=http://nlp.seas.harvard.edu/annotated-transformer/" >代码详细实现 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
<li>BERT大火却不懂Transformer？读这一篇就够了 - 数据汪的文章 - 知乎 <a class="link"   target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/54356280" >https://zhuanlan.zhihu.com/p/54356280 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
<li><strong>Attention Mechanism and Softmax</strong><a class="link"   href="https://link.zhihu.com/?target=https://medium.com/analytics-vidhya/attention-mechanism-and-softmax-65d8d50f7786" >链接 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
<li>神经网络训练trick - Anticoder的文章 - 知乎 <a class="link"   target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/59918821" >https://zhuanlan.zhihu.com/p/59918821 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
<li><a class="link"   href="https://link.zhihu.com/?target=https://www.jianshu.com/p/e454b17aa9be" >神经网络训练 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
<li><a class="link"   href="https://link.zhihu.com/?target=https://datascience.stackexchange.com/questions/69640/what-should-be-the-labels-for-subword-tokens-in-bert-for-ner-task" >What should be the labels for subword tokens in BERT for NER task? <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
<li>面试问题: <a class="link"   target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/363466672" >链接 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
<li>Transformer模型原理详解：<a class="link"   target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/44121378" >https://zhuanlan.zhihu.com/p/44121378 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
<li>原理详解还有<a class="link"   target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/48508221" >https://zhuanlan.zhihu.com/p/48508221 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
</ul>
<hr>
<p><strong>问题：</strong></p>
<p><strong>self-attention 比CNN更好?为什么好?怎么做的改进?创新点?改进的思路在哪里?为什么要从CNN到transformer?</strong></p>
<p>CNN要实现长距离依赖需要间隔trick或者模型更深, 对比起来self-attention捕获长距离依赖更直接以及花费相对更少, 并且实验效果表示出self-attention为基础的transformer表现更好.<strong>Attention Is All You Need</strong></p>

        </div>

        
            <div class="post-copyright-info w-full my-8 px-2 sm:px-6 md:px-8">
                <div class="article-copyright-info-container">
    <ul>
        <li><strong>Title:</strong> Articles</li>
        <li><strong>Author:</strong> Chiyomi</li>
        <li><strong>Created at
                :</strong> 2024-08-02 20:46:39</li>
        
            <li>
                <strong>Updated at
                    :</strong> 2024-08-03 17:10:54
            </li>
        
        <li>
            <strong>Link:</strong> https://redefine.ohevan.com/2024/08/02/Articles/
        </li>
        <li>
            <strong>
                License:
            </strong>
            

            
                This work is licensed under <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0">CC BY-NC-SA 4.0</a>.
            
        </li>
    </ul>
</div>

            </div>
        

        

        

        
            <div class="article-nav my-8 flex justify-between items-center px-2 sm:px-6 md:px-8">
                
                
                    <div class="article-next border-border-color shadow-redefine-flat shadow-shadow-color-2 rounded-medium px-4 py-2 hover:shadow-redefine-flat-hover hover:shadow-shadow-color-2">
                        <a class="next"
                        rel="next"
                        href="/2024/07/19/text1/"
                        >
                            <span class="title flex justify-center items-center">
                                <span class="post-nav-title-item">text1</span>
                                <span class="post-nav-item">Next posts</span>
                            </span>
                            <span class="right arrow-icon flex justify-center items-center">
                                <i class="fa-solid fa-chevron-right"></i>
                            </span>
                        </a>
                    </div>
                
            </div>
        


        
            <div class="comment-container px-2 sm:px-6 md:px-8 pb-8">
                <div class="comments-container mt-10 w-full ">
    <div id="comment-anchor" class="w-full h-2.5"></div>
    <div class="comment-area-title w-full my-1.5 md:my-2.5 text-xl md:text-3xl font-bold">
        Comments
    </div>
    

        
            
    <div id="waline"></div>
    <script type="module" data-swup-reload-script>
      import { init } from '/js/libs/waline.mjs';

      function loadWaline() {
        init({
          el: '#waline',
          serverURL: 'https://example.example.com',
          lang: 'zh-CN',
          dark: 'body[class~="dark-mode"]',
          reaction: false,
          requiredMeta: ['nick', 'mail'],
          emoji: [],
          recaptchaV3Key: "wasd",
          
        });
      }

      if (typeof swup !== 'undefined') {
        loadWaline();
      } else {
        window.addEventListener('DOMContentLoaded', loadWaline);
      }
    </script>



        
    
</div>

            </div>
        
    </div>

    
        <div class="toc-content-container">
            <div class="post-toc-wrap">
    <div class="post-toc">
        <div class="toc-title">On this page</div>
        <div class="page-title">Articles</div>
        <ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Attention-is-all-you-need-%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB"><span class="nav-text">Attention is all you need-论文精读</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-text">前言</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#RNN"><span class="nav-text">RNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LSTM%EF%BC%9A"><span class="nav-text">LSTM：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CNN%EF%BC%9A"><span class="nav-text">CNN：</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%91%98%E8%A6%81"><span class="nav-text">摘要</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E4%BB%8B%E7%BB%8D"><span class="nav-text">1.介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E8%83%8C%E6%99%AF"><span class="nav-text">2.背景</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84"><span class="nav-text">3.模型架构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-Encode-and-Decoder-Stacks"><span class="nav-text">3.1 Encode and Decoder Stacks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-Attention"><span class="nav-text">3.2 Attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-Position-wise-Feed-Forward-Networks"><span class="nav-text">3.3 Position-wise Feed-Forward Networks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-Embeddings-and-Softmax"><span class="nav-text">3.4 Embeddings and Softmax</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-Positional-Encoding"><span class="nav-text">3.5 Positional Encoding</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-Why-Self-Attention"><span class="nav-text">4 Why Self-Attention</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-Training"><span class="nav-text">5 Training</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-Training-Data-and-Batching"><span class="nav-text">5.1 Training Data and Batching</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83"><span class="nav-text">参考</span></a></li></ol></li></ol>

    </div>
</div>
        </div>
    
</div>



                

            </div>

            

        </div>

        <div class="main-content-footer">
            <footer class="footer mt-5 py-5 h-auto text-base text-third-text-color relative border-t-2 border-t-border-color">
    <div class="info-container py-3 text-center">
        
        <div class="text-center">
            &copy;
            
              <span>2024</span>
              -
            
            2024&nbsp;&nbsp;<i class="fa-solid fa-heart fa-beat" style="--fa-animation-duration: 0.5s; color: #f54545"></i>&nbsp;&nbsp;<a href="/">Chiyomi</a>
            
                
                <p class="post-count space-x-0.5">
                    <span>
                        2 posts in total
                    </span>
                    
                </p>
            
        </div>
        
            <script data-swup-reload-script src="https://cn.vercount.one/js"></script>
            <div class="relative text-center lg:absolute lg:right-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-right">
                
                    <span id="busuanzi_container_site_uv" class="lg:!block">
                        <span class="text-sm">VISITOR COUNT</span>
                        <span id="busuanzi_value_site_uv"></span>
                    </span>
                
                
                    <span id="busuanzi_container_site_pv" class="lg:!block">
                        <span class="text-sm">TOTAL PAGE VIEWS</span>
                        <span id="busuanzi_value_site_pv"></span>
                    </span>
                
            </div>
        
        <div class="relative text-center lg:absolute lg:left-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-left">
            <span class="lg:block text-sm">POWERED BY <?xml version="1.0" encoding="utf-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg class="relative top-[2px] inline-block align-baseline" version="1.1" id="圖層_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="1rem" height="1rem" viewBox="0 0 512 512" enable-background="new 0 0 512 512" xml:space="preserve"><path fill="#0E83CD" d="M256.4,25.8l-200,115.5L56,371.5l199.6,114.7l200-115.5l0.4-230.2L256.4,25.8z M349,354.6l-18.4,10.7l-18.6-11V275H200v79.6l-18.4,10.7l-18.6-11v-197l18.5-10.6l18.5,10.8V237h112v-79.6l18.5-10.6l18.5,10.8V354.6z"/></svg><a target="_blank" class="text-base" href="https://hexo.io">Hexo</a></span>
            <span class="text-sm lg:block">THEME&nbsp;<a class="text-base" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v2.6.4</a></span>
        </div>
        
        
            <div>
                Blog up for <span class="odometer" id="runtime_days" ></span> days <span class="odometer" id="runtime_hours"></span> hrs <span class="odometer" id="runtime_minutes"></span> Min <span class="odometer" id="runtime_seconds"></span> Sec
            </div>
        
        
            <script data-swup-reload-script>
                try {
                    function odometer_init() {
                    const elements = document.querySelectorAll('.odometer');
                    elements.forEach(el => {
                        new Odometer({
                            el,
                            format: '( ddd).dd',
                            duration: 200
                        });
                    });
                    }
                    odometer_init();
                } catch (error) {}
            </script>
        
        
        
    </div>  
</footer>
        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="article-tools-list">
        <!-- TOC aside toggle -->
        
            <li class="right-bottom-tools page-aside-toggle">
                <i class="fa-regular fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
            <li class="go-comment">
                <i class="fa-regular fa-comments"></i>
            </li>
        
    </ul>
</div>

        </div>
    

    <div class="right-side-tools-container">
        <div class="side-tools-container">
    <ul class="hidden-tools-list">
        <li class="right-bottom-tools tool-font-adjust-plus flex justify-center items-center">
            <i class="fa-regular fa-magnifying-glass-plus"></i>
        </li>

        <li class="right-bottom-tools tool-font-adjust-minus flex justify-center items-center">
            <i class="fa-regular fa-magnifying-glass-minus"></i>
        </li>

        <li class="right-bottom-tools tool-dark-light-toggle flex justify-center items-center">
            <i class="fa-regular fa-moon"></i>
        </li>

        <!-- rss -->
        

        

        <li class="right-bottom-tools tool-scroll-to-bottom flex justify-center items-center">
            <i class="fa-regular fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="visible-tools-list">
        <li class="right-bottom-tools toggle-tools-list flex justify-center items-center">
            <i class="fa-regular fa-cog fa-spin"></i>
        </li>
        
            <li class="right-bottom-tools tool-scroll-to-top flex justify-center items-center">
                <i class="arrow-up fas fa-arrow-up"></i>
                <span class="percent"></span>
            </li>
        
        
    </ul>
</div>

    </div>

    <div class="image-viewer-container">
    <img src="">
</div>


    

</main>


    
<script src="/js/libs/Swup.min.js"></script>

<script src="/js/libs/SwupSlideTheme.min.js"></script>

<script src="/js/libs/SwupScriptsPlugin.min.js"></script>

<script src="/js/libs/SwupProgressPlugin.min.js"></script>

<script src="/js/libs/SwupScrollPlugin.min.js"></script>

<script src="/js/libs/SwupPreloadPlugin.min.js"></script>

<script>
    const swup = new Swup({
        plugins: [
            new SwupScriptsPlugin({
                optin: true,
            }),
            new SwupProgressPlugin(),
            new SwupScrollPlugin({
                offset: 80,
            }),
            new SwupSlideTheme({
                mainElement: ".main-content-body",
            }),
            new SwupPreloadPlugin(),
        ],
        containers: ["#swup"],
    });
</script>







<script src="/js/tools/imageViewer.js" type="module"></script>

<script src="/js/utils.js" type="module"></script>

<script src="/js/main.js" type="module"></script>

<script src="/js/layouts/navbarShrink.js" type="module"></script>

<script src="/js/tools/scrollTopBottom.js" type="module"></script>

<script src="/js/tools/lightDarkSwitch.js" type="module"></script>

<script src="/js/layouts/categoryList.js" type="module"></script>





    
<script src="/js/tools/codeBlock.js" type="module"></script>




    
<script src="/js/layouts/lazyload.js" type="module"></script>




    
<script src="/js/tools/runtime.js"></script>

    
<script src="/js/libs/odometer.min.js"></script>

    
<link rel="stylesheet" href="/assets/odometer-theme-minimal.css">




  
<script src="/js/libs/Typed.min.js"></script>

  
<script src="/js/plugins/typed.js" type="module"></script>









<div class="post-scripts" data-swup-reload-script>
    
        
<script src="/js/tools/tocToggle.js" type="module"></script>

<script src="/js/layouts/toc.js" type="module"></script>

<script src="/js/plugins/tabs.js" type="module"></script>

    
</div>


</body>
</html>
