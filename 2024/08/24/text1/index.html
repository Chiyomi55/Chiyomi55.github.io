<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>特征工程入门学习记录 | Memoryβ</title><meta name="author" content="Chiyomi"><meta name="copyright" content="Chiyomi"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="用于存放入门学习的一些笔记，有很多处是搬运参考文章内容，主要目的仅是便于记录学习过程和方便回顾查看。 ……………………………………………………………………………………………………….. …………………………………………………………………………………………………………………………………………………………………………………………………………………. 1.特征预处理1.1缺失值处理 实际上我们收集到的很">
<meta property="og:type" content="article">
<meta property="og:title" content="特征工程入门学习记录">
<meta property="og:url" content="http://example.com/2024/08/24/text1/index.html">
<meta property="og:site_name" content="Memoryβ">
<meta property="og:description" content="用于存放入门学习的一些笔记，有很多处是搬运参考文章内容，主要目的仅是便于记录学习过程和方便回顾查看。 ……………………………………………………………………………………………………….. …………………………………………………………………………………………………………………………………………………………………………………………………………………. 1.特征预处理1.1缺失值处理 实际上我们收集到的很">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/img/butterfly-icon.png">
<meta property="article:published_time" content="2024-08-24T05:05:28.000Z">
<meta property="article:modified_time" content="2024-08-24T05:12:12.567Z">
<meta property="article:author" content="Chiyomi">
<meta property="article:tag" content="特征工程">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/butterfly-icon.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2024/08/24/text1/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        if (name && globalFn[key][name]) return
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '特征工程入门学习记录',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-08-24 13:12:12'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Memoryβ</span></a><a class="nav-page-title" href="/"><span class="site-name">特征工程入门学习记录</span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">特征工程入门学习记录</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-08-24T05:05:28.000Z" title="Created 2024-08-24 13:05:28">2024-08-24</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2024-08-24T05:12:12.567Z" title="Updated 2024-08-24 13:12:12">2024-08-24</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>用于存放入门学习的一些笔记，有很多处是搬运参考文章内容，主要目的仅是便于记录学习过程和方便回顾查看。</p>
<p>………………………………………………………………………………………………………..</p>
<p>………………………………………………………………………………………………………………………………………………………………………………………………………………….</p>
<h2 id="1-特征预处理"><a href="#1-特征预处理" class="headerlink" title="1.特征预处理"></a>1.特征预处理</h2><h3 id="1-1缺失值处理"><a href="#1-1缺失值处理" class="headerlink" title="1.1缺失值处理"></a>1.1缺失值处理</h3><p> 实际上我们收集到的很多数据是存在缺失值的，比如某个视频缺少总时长，对于用户属性数据来说，很多用户可能也不会填写完备的信息。一般缺失值可以用均值、中位数、众数等填充，或者直接将缺失值当做一个特定的值来对待。还可以利用一些复杂的插值方法，如<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E6%A0%B7%E6%9D%A1%E6%8F%92%E5%80%BC">样条插值</a>等来填充缺失值。 </p>
<h3 id="1-2归一化"><a href="#1-2归一化" class="headerlink" title="1.2归一化"></a>1.2归一化</h3><p> 不同特征之间由于量纲不一样，数值可能相差很大，直接将这些差别极大的特征灌入模型，会导致数值小的特征根本不起作用，一般我们要对数值特征进行归一化处理，常用的归一化方法有min-max归一化、分位数归一化、正态分布归一化、行归一化等。 </p>
<p> min-max归一化是通过求得该特征样本的最大值和最小值，采用如下公式来进行归一化，归一化后所有值分布在0-1之间。 </p>
<p> 分位数归一化，是将该特征所有的值从小到大排序，假设一共有N个样本，某个值x排在第k位，那么我们用下式来表示x的新值。 </p>
<p> 正态分布归一化，是通过求出该特征所有样本值的均值μ和<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E6%A0%87%E5%87%86%E5%B7%AE">标准差</a> σ，再采用下式来进行归一化。 </p>
<h3 id="1-3异常值与数值截断"><a href="#1-3异常值与数值截断" class="headerlink" title="1.3异常值与数值截断"></a>1.3异常值与数值截断</h3><p>对于数值型特征，可能会存在异常值，包括异常大和异常小的值。在统计数据处理中有所谓3σ准则，即对于服从正态分布的随机变量，该变量的数值分布在（μ-3σ,μ+3σ)中的概率为0.9974，这时可以将超出该范围的值看成异常值，采用向上截断(用μ-3σ)和向下截断(用μ+3σ)的方法来为异常值赋予新的值。对于真实业务场景，可能还要根据<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E7%89%B9%E5%BE%81%E5%8F%98%E9%87%8F">特征变量</a>的实际意义来进行处理，在我们做视频推荐过程中，经常会发现日志中视频的总时长是一个非常非常大的值(可能是在日志埋点时将时间戳混杂到时长中了)，我们一般会用户180分钟来截断电影的总时长，用45分钟来截断电视剧单集的总时长。</p>
<p>如果异常值所占样本比例非常小，也可以直接将包含异常值的样本剔除掉，但是有很多真实业务场景利用非常多的特征，虽然每个特征异常值很少，但是如果特征总数很多的话，包含异常值的样本(只要包含某一个异常值的都算异常样本)总数可能是非常大的，所以直接丢弃的方法有时是不合适的。</p>
<h3 id="1-4非线性变换："><a href="#1-4非线性变换：" class="headerlink" title="1.4非线性变换："></a>1.4非线性变换：</h3><p> 有时某个属性不同值之间差别较大(比如年收入)，有时为了让模型具备更多的非线性能力(特别是对于线性模型)，这两种情况下都需要对特征进行非线性变换，比如值取对数(值都是正的情况下)作为最终的特征，也可以采用多项式、高斯变换、logistic变换等转化为非线性特征。上面提到的分位数归一化、正态分布归一化其实都是非线性变换。 </p>
<h2 id="2-特征构建"><a href="#2-特征构建" class="headerlink" title="2.特征构建"></a>2.特征构建</h2><p> 所谓特征构建是从原始数据中提取特征，将原始数据空间映射到新的特征<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E5%90%91%E9%87%8F%E7%A9%BA%E9%97%B4">向量空间</a>，使得在新的特征空间中，模型能够更好地学习数据中的规律。下面我们分别对前面提到的<strong>离散(类别) 特征、连续(数值)特征、时空特征、文本特征、富媒体特征等5类非常重要的特征</strong>来介绍怎么从原始数据构建相关的特征。随着Word2Vec及深度学习技术在推荐系统中的大规模应用，嵌入方法越来越受到欢迎，我们也会单独讲一下嵌入特征，文本、富媒体一般可以转化为嵌入特征。 </p>
<h3 id="2-1离散特征"><a href="#2-1离散特征" class="headerlink" title="2.1离散特征"></a>2.1离散特征</h3><p>one-hot编码</p>
<p>one-hot编码通常用于类别特征，如果某个类别特征有k类，我们将这k类固定一个序关系(随便什么序关系都无所谓，只是方便确认某个类在哪个位置)，我们可以将每个值映射为一个k维向量，其中这个值所在的分量为1，其他分量为0。该方法当类别的数量很多时，特征空间会变得非常大。在这种情况下，一般可以用 PCA 等方法进行降维。</p>
<p>对于标签这种类别特征，可能每个视频有多个标签，这时one-hot编码可以拓展为n-hot编码，就是该视频在它包含的所有标签对应的分量为1，其他为0。</p>
<p><strong>1、散列编码</strong></p>
<p> 对于有些取值特别多的类别特征(比如视频标签，可以有几万个)，使用one-hot编码得到的特征矩阵非常稀疏，如果再进行特征交叉，会使得特征维度爆炸式增长。特征散列的目标就是是把原始的高维特征向量压缩成较低维特征向量，且尽量不损失原始特征的表达能力，其优势在于实现简单，所需额外计算量小。降低特征维度，也能加速算法训练与预测，降低内存消耗，但代价是通过哈希转换后学习到的模型变得很难检验(因为一般<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E5%93%88%E5%B8%8C%E5%87%BD%E6%95%B0">哈希函数</a>是不可逆的)，我们很难对训练出的模型参数做出合理解释。特征散列的另一个问题是可能把多个原始特征哈希到相同的位置上，出现哈希冲突现象，但经验表明这种冲突对算法的精度影响很小，通过选择合适的<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=hash%E5%87%BD%E6%95%B0">hash函数</a>也可以减少冲突概率。 </p>
<p><strong>2、计数编码</strong></p>
<p> 就是将所有样本中该类别出现的次数或者频次作为该值的编码，这类方法对异常值比较敏感(拿电影的标签来说，很多电影包含“剧情”这个标签，计数编码会让剧情的编码值非常大)，也容易产生冲突(两个不同类别的编码一样，特别是对于出现很稀少的标签，编码值一样的概率非常大)。 </p>
<p><strong>3、离散特征之间交叉</strong></p>
<p>就是类别特征之间通过笛卡尔积(或者笛卡尔积的一个子集)生成新的特征，通过特征交叉有时可以捕捉细致的信息，对模型预测起到很重要的作用。这里举个例子，比如用用户地域与视频语言做交叉，大家肯定知道广东人一般更喜欢看粤语剧，那么这个交叉特征对预测粤语视频的点击是非常有帮助的。类别交叉一般需要对业务有较好的理解，需要足够多的领域知识，才可以构建好的交叉特征。</p>
<p>上面讲的是2个类别特征的交叉，当然还可以做3个、4个、甚至更多类别特征的交叉，两个类别交叉最多可以产生这两个类别基数的乘积这么多的新特征，所以交叉让模型的维数爆炸性增长，增加了模型训练的难度。同时，更多的特征需要更多的样本来支撑，否则极容易<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E8%BF%87%E6%8B%9F%E5%90%88">过拟合</a>。对于样本量不够多的场景，不建议采用超出2个类别的交叉，也不建议用2个基数特别大的类别进行特征交叉。</p>
<h3 id="2-2连续（数值）特征"><a href="#2-2连续（数值）特征" class="headerlink" title="2.2连续（数值）特征"></a>2.2连续（数值）特征</h3><p><strong>1、直接使用</strong></p>
<p>机器学习算法是可以直接处理数值特征的，数值特征可能经过上面一节讲的特征预处理中的部分步骤再灌给模型使用。 </p>
<p><strong>2、离散化</strong></p>
<p>有时连续特征需要进行离散化处理，比如视频在一段时间内的播放量对于视频点击CTR预估可能是一个重要的特征，因为播放次数跟视频的热度有很强的相关性，但是如果不同视频的播放次数的数量级相差巨大(实际情况确实是这样，热门视频比冷门视频播放量大若干个数量级)，该特征就很难起作用(比如 LR 模型，模型往往只对比较大的特征值敏感)。对于这种情况，通常的解决方法是进行分桶。分桶操作可以看作是对数值变量的离散化，之后再进行 one-hot 编码。</p>
<p>分桶的数量和宽度可以根据业务知识和经验来确定，一般有三种分桶方式：(1)等距分桶，每个桶的长度是固定的，这种方式适用于样本分布比较均匀的情况；(2)等频分桶，即每个桶里样本量一样多，但也会出现特征值差异非常大的样本被放在一个桶中的情况；(3)模型分桶，使用模型找到最佳分桶，例如利用聚类的方式将特征分成多个类别，或者利用树模型，这种<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B">非线性模型</a>天生具有对连续型特征切分的能力，利用特征分割点进行离散化。</p>
<p>分桶是离散化的常用方法，连续特征离散化是有一定价值的：离散化之后得到的稀疏向量，运算速度更快，计算结果易于存储。离散化之后的特征对于异常值也具有更强的<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E9%B2%81%E6%A3%92%E6%80%A7">鲁棒性</a>。需要注意的是：1)每个桶内都有足够多的样本，否则不具有统计意义；2)每个桶内的样本尽量分布均匀。</p>
<p><strong>3、特征交叉</strong></p>
<p> 对于连续特征x、y，通过<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%87%BD%E6%95%B0">非线性函数</a>f的作用，我们将z&#x3D;f(x,y)作为交叉特征，一般f可以是多项式函数，最常用的交叉函数是f&#x3D;xy，即两个特征对应的值直接相乘。通过特征交叉可以为模块提供更多的非线性，可以更细致地拟合输入输出之间的复杂关系，但非线性交叉让模型计算处理变得更加困难。 </p>
<h3 id="2-3时空特征"><a href="#2-3时空特征" class="headerlink" title="2.3时空特征"></a>2.3时空特征</h3><p><strong>1、转化为数值</strong></p>
<p> 比如将时间转化为从某个基准时间开始到该时间经历的秒数、天数、月数、年数等。用更大的单位相当于对小单位四舍五入(比如用到当前时间经历的年数，那么不足一年的时间都忽略了)。 </p>
<p><strong>2、将时间离散化</strong></p>
<p>比如我们可以根据当前时间是不是节假日，将时间离散化为0-1二值(1是假日，0是工作日)。再比如如果我们构建的模型是与周期性相关的，我们可能只需要取时间中的周几这个量，那么时间就可以离散化为0-6七个数字(0代表星期天，1代表星期一，如此类推)。</p>
<p>对于地理位置来说，我们有行政区划表示，还有经纬度表示，以及到某个固定点的距离等表示方式。</p>
<h3 id="2-4文本特征"><a href="#2-4文本特征" class="headerlink" title="2.4文本特征"></a>2.4文本特征</h3><p> 对于文本一般可以用NLP相关技术进行相关处理转化为数值特征。对于新闻资讯等文档，可以采用TF-IDF、LDA等将每篇文档转化为一个高维的向量表示。或者基于Word2Vec等相关技术将整篇文档嵌入(doc2vec)到一个低维的稠密向量空间。 </p>
<h3 id="2-5富媒体特征"><a href="#2-5富媒体特征" class="headerlink" title="2.5富媒体特征"></a>2.5富媒体特征</h3><p> 对于图片、音频、视频等富媒体，一般也可以基于相关领域的技术获得对应的向量表示，这种向量表示就可以作为富媒体的特征了。这里不细介绍，感兴趣的读者可以自行搜素学习。 </p>
<h3 id="2-6嵌入特征"><a href="#2-6嵌入特征" class="headerlink" title="2.6嵌入特征"></a>2.6嵌入特征</h3><p>上面文本、富媒体中提到的嵌入技术是非常重要的一类提取特征的技术，所谓嵌入，就是将<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E9%AB%98%E7%BB%B4%E7%A9%BA%E9%97%B4">高维空间</a>的向量投影到低维空间，降低数据的稀疏性，减少<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E7%BB%B4%E6%95%B0%E7%81%BE%E9%9A%BE">维数灾难</a>，同时提升数据表达的鲁棒性。随着Word2Vec及深度学习技术的流行，嵌入特征越来越重要。</p>
<p>标的物嵌入分为基于内容的嵌入和基于行为的嵌入。前者使用标的物属性信息(如视频的标题、标签、演职员、海报图，视频、音频等信息)，通过 NLP、CV、深度学习等技术生成嵌入向量，后者是基于用户与标的物的交互行为数据生成嵌入，拿视频来举例，用户在一段时间中前后点击的视频存在一定的相似性，通常会表现出对某类型视频的兴趣偏好，可能是同一个风格类别，或者是相似的话题等，因此我们将一段时间内用户点击的视频 id 序列作为训练数据(id可以类比word，这个序列类比为一篇文档)，使用 skip-gram 模型学习视频的嵌入特征。由于用户点击行为具有相关关系，因此得到的嵌入特征有很好的聚类效果，使得在特征空间中，同类目的视频聚集在一起，相似类目的视频在空间中距离相近。在电视猫的相似视频推荐中，我们就是采用这种嵌入，并且聚类，将同一类别中的其他视频作为相关推荐的，效果还是非常好的。</p>
<p>对于用户嵌入，我们也可以有很多方法，下面是两种比较基础的方法。</p>
<p>可以将一段时间内用户点击过的视频的平均嵌入特征向量作为该用户的嵌入特征，这里的“平均”可以是简单的算术平均，可以是element-wise max，也可以是根据视频的热度和时间属性等进行加权平均或者尝试用 RNN 替换掉平均操作。我们可以通过选择时间周期的长短来刻画用户的长期兴趣嵌入和短期兴趣嵌入。</p>
<p>另外参考YouTube推荐系统(见<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE">参考文献</a>6)的思路，我们可以把推荐问题等价为一个多类别分类问题，使用 softmax <a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0">损失函数</a>学习一个 DNN 模型，最终预测在某一时刻某一上下文信息下用户观看的下一个视频的类别，最后把训练好的 DNN 模型最后一层隐含层输出作为用户的嵌入。</p>
<h2 id="3-特征选择"><a href="#3-特征选择" class="headerlink" title="3.特征选择"></a>3.特征选择</h2><p>特征选择是指从所有构建的特征中选择出一个子集，用于模型训练与学习的过程。特征选择不光要评估特征本身，更需要评估特征与模型的匹配度，评估特征对最终的预测目标的精准度的贡献。特征没有最好的，只有跟应用场景和模型合适的，特征选择对于构建机器学习应用是非常重要的一环。特征选择主要有以下两个目的：</p>
<ul>
<li><strong>简化模型，节省存储和计算开销，让模型更易于理解和使用；</strong></li>
<li><strong>减少特征数量、降维，改善通用性、降低过拟合的风险。</strong></li>
</ul>
<p>知道了什么是特征选择以及特征选择的价值，下面我们提供进行特征选择的具体方法，主要有基于统计量的方法和基于模型的方法。</p>
<h3 id="3-1基于统计量选择"><a href="#3-1基于统计量选择" class="headerlink" title="3.1基于统计量选择"></a>3.1基于统计量选择</h3><p>基于统计量选择主要有如下几种方式，我们分别介绍：</p>
<p><strong>(1) 选择方差大的特征</strong></p>
<p>方差反应了特征样本的分布情况，我们可以分析特征的数据分布，分布均匀的特征，样本之间差别不大，该特征不能很好区分不同样本，而分布不均匀的特征，样本之间有极大的区分度，因此通常可以选择方差较大的特征，去掉方差变化小的特征。具体方差多大算大，可以事先计算出所有特征的方差，选择一定比例(比如20%)的方差大的特征，或者可以设定一个阈值，选择方差大于阈值的特征。</p>
<p><strong>(2) <a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E7%9A%AE%E5%B0%94%E9%80%8A%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0">皮尔逊相关系数</a></strong></p>
<p><a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E7%9A%AE%E5%B0%94%E6%A3%AE">皮尔森</a>相关系数是一种简单的，能帮助理解特征和目标变量之间关系的方法，用于衡量变量之间的<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E7%BA%BF%E6%80%A7%E7%9B%B8%E5%85%B3%E6%80%A7">线性相关性</a>，取值区间为[-1，1]，-1 表示完全的负相关，+1 表示完全的正相关，0 表示没有线性关系。通过分析特征与目标之间的相关性，优先选择与目标相关性高的特征。如果两个特征之间线性相关度的绝对值大，说明这两个特征是有很强的相关关系的，我们没必要都选择，只需要选择其中一个即可。</p>
<p><strong>(3) 覆盖率</strong></p>
<p>特征的覆盖率是指训练样本中有多大比例的样本具备该特征。我们首先计算每个特征的覆盖率，覆盖率很小的特征对模型的预测效果作用不大，可以剔除。</p>
<p><strong>(4) <a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C">假设检验</a></strong></p>
<p>假设特征变量和目标变量之间相互独立，选择适当检验方法计算统计量，然后根据统计量做出统计推断。例如对于特征变量为类别变量而目标变量为连续数值变量的情况，可以使用<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E6%96%B9%E5%B7%AE%E5%88%86%E6%9E%90">方差分析</a>，对于特征变量和目标变量都为连续数值变量的情况，可以使用皮尔森卡方检验。卡方统计量取值越大，特征相关性越高。</p>
<p><strong>(5) 互信息</strong></p>
<p>在概率论和<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E4%BF%A1%E6%81%AF%E8%AE%BA">信息论</a>中，互信息用来度量两个变量之间的相关性。互信息越大则表明两个变量相关性越高，互信息为 0 时，两个变量相互独立。因此可以根据特征变量和目标变量之间的互信息来选择互信息大的特征。</p>
<p><strong>b 基于模型选择</strong></p>
<p>基于模型的特征选择，可以直接根据模型参数来选择，也可用子集选择的思路选出特征的最优组合。</p>
<p><strong>(1) 基于模型参数</strong></p>
<p>对于线性模型，可以直接基于模型系数大小来决定特征的重要程度。对于树模型，如决策树、梯度提升树、<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97">随机森林</a>等，每一颗树的生成过程，都对应了一个特征选择的过程，在每次选择分类节点时，都会选择最佳分类特征来进行切分，重要的特征更有可能出现在树生成早期的节点，作为分裂节点的次数也越多。因此，可以基于树模型中特征出现次数等指标对特征重要性进行排序。如果我们想要得到稀疏特征或者说是对特征进行降维，可以在模型上主动使用正则化技术。使用L1正则，调整正则项的权重，基本可以得到任意维度的稀疏特征。</p>
<p><strong>(2) 子集选择</strong></p>
<p>基于模型，我们也可以用子集选择的思路来选取特征。常见的有前向搜索和反向搜索两种思路。如果我们先从N个特征中选出一个最好的特征，然后让其余的N-1个特征分别与第一次选出的特征进行组合，从N-1个二元特征组合中选出最优组合，然后在上次的基础上，添加另一个新的特征，考虑3个特征的组合，依次类推，这种方法叫做前向搜索。反之，如果我们的目标是每次从已有特征中去掉一个特征，并从这些组合中选出最优组合，这种方法就是反向搜索。如果特征数量较多、模型复杂，那么这种选择的过程是非常耗时间和资源的。我们在后面讲到的自动特征工程中会提到通过算法自动选特征的技术方案。</p>
<p><strong>3.4 特征评估</strong></p>
<p>所谓特征评估是在将特征灌入模型进行训练之前，事先评估特征的价值，提前发现可能存在的问题，及时解决，避免将有问题的特征导入模型，导致训练过程冗长而得不到好的结果。特征评估是<strong>对选择好的特征进行整体评价</strong>，而不是特征选择中所谓的对单个特征重要性的评判。特征评估包括特征的覆盖率、特征的维度、定性分析和定量分析等几种方式。</p>
<p>特征的覆盖率是指有多少比例的样本可以构建出相关特征，对于推荐系统来说，存在用户冷启动，因此对于新用户，如果选择的特征中包含从用户行为中获得的特征，那么我们是无法为他构建特征的，从而无法利用模型来为他进行推荐。</p>
<p>特征的维度衡量的是模型的表达能力，维度越高，模型表达能力越强，这时就需要更多的样本量和更多的计算资源、优秀的<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97">分布式计算</a>框架来支撑模型的训练。为了达到较好的训练效果，一般对于简单模型可以用更多维度的特征，而对于复杂模型可以用更少的维度。</p>
<p>定性分析是指构建的特征是否跟用户行为是冲突的，可以拿熟悉的样本来做验证，比如在视频推荐中，可以根据自己的行为来定性验证标签的正确性。我个人最喜欢看恐怖电影，那么基于标签构建特征的话，那么对于我的样本，在恐怖这个标签上的权重应该是比其他标签权重大的。</p>
<p>定量分析，通过常用的离线评估指标，如Precitson、Recall、AUC等等来验证模型的效果，当然，最终需要上线做AB测试来看是否对核心用户体验、商业化指标有提升。</p>
<h2 id="4-常用推荐算法"><a href="#4-常用推荐算法" class="headerlink" title="4.常用推荐算法"></a>4.常用推荐算法</h2><p>在本节我们基于3类主流的推荐产品形态来介绍与之相关的数据与特征工程。不同产品形态可以有不同的推荐算法实现，我们会根据常用的推荐算法来说明需要什么样的数据，以及怎么基于这些数据通过特征工程来构建特征用于推荐模型训练。</p>
<h3 id="4-1-排行榜推荐"><a href="#4-1-排行榜推荐" class="headerlink" title="4.1 排行榜推荐"></a><strong>4.1 排行榜推荐</strong></h3><p>排行榜推荐是非个性化推荐，也是最简单的一类推荐算法，一般可以作为独立产品或者作为其他个性化推荐算法的默认推荐。排行榜推荐一般只依赖于用户行为数据，根据用户在过去一段时间(比如过去一周、一月、一年等)的操作行为或者标的物自身相关的属性特征，基于某个维度统计最受欢迎的标的物列表作为推荐列表。常用的有最新榜(这个是根据标的物上线时间来统计的)、最热榜、收藏榜、热销榜等等。</p>
<p>这类推荐算法也不需要什么复杂特征工程，只需要对数据进行简单的统计处理就可以了，一般用select sum(N) as num from T group by sid order by num desc limit 100这类SQL就可以搞定，这里N是标的物的播放量、销量等，T是通过数据预处理生成的结构化的表，sid是标的物的唯一编码id)。需要注意的是，对于不同类别的标的物(如电影和电视剧)，如果要混合排序的话，需要将统计量(如播放量)归一化到同一个可比较的范围中。</p>
<h3 id="4-2-标的物关联标的物推荐"><a href="#4-2-标的物关联标的物推荐" class="headerlink" title="4.2 标的物关联标的物推荐"></a><strong>4.2 标的物关联标的物推荐</strong></h3><p>标的物关联标的物推荐，就是我们通常所说的关联推荐，给每个标的物关联一系列相关的标的物作为推荐列表。根据所使用算法不同，对数据源、数据处理及特征工程有不同的要求。下面分别介绍。</p>
<h4 id="4-2-1-基于内容的关联推荐"><a href="#4-2-1-基于内容的关联推荐" class="headerlink" title="4.2.1 基于内容的关联推荐"></a><strong>4.2.1 基于内容的关联推荐</strong></h4><p>如果是新闻资讯类标的物，一般我们可以用TF-IDF算法来向量化每篇文章，这样就可以根据<strong>向量相似度</strong>来计算两篇文章的相似度了。这时我们利用的数据就是标的物本身的文本信息，该算法也没有复杂的特征工程，只需将所有文章进行分词，构建词库，剔除<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E5%81%9C%E7%94%A8%E8%AF%8D">停用词</a>，得到最终所有词的corpora，对每篇文章利用TF-IDF算法获得每篇文章的向量表示，这个向量就可以认为是是最终的特征。</p>
<p>当然也可以利用向量空间模型，将标的物的每一个属性转换为一个特征，进而获得特征表示。如果标的物包含标签信息，我们可以用每一个标签表示一个特征，采用one-hot编码的方式构建特征表示。对于标的物图片、视频、音频数据也可以采用嵌入技术构建特征向量。</p>
<h4 id="4-2-2-基于协同过滤的关联推荐"><a href="#4-2-2-基于协同过滤的关联推荐" class="headerlink" title="4.2.2 基于协同过滤的关联推荐"></a><strong>4.2.2 基于<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4">协同过滤</a>的关联推荐</strong></h4><p>传统的item-based协同过滤，通过获取用户操作行为，构建<strong>用户行为矩阵</strong>，该矩阵的每一列就是某个item的特征向量表示，基于该向量就可以根据cosine余弦计算相似度。基于矩阵分解的协同过滤，也是通过用户行为矩阵进行矩阵分解，获得标的物特征矩阵，该矩阵的每一列就是标的物的特征表示。这两类方法，只需要利用用户行为数据，获得的标的物向量表示就是最终的特征，所以也不存在复杂的特征工程。</p>
<p>还有一种基于item2vec的方法基于用户行为数据来获得标的物的嵌入表示，也不需要进行复杂的特征工程。</p>
<h3 id="4-3-个性化推荐"><a href="#4-3-个性化推荐" class="headerlink" title="4.3 个性化推荐"></a><strong>4.3 个性化推荐</strong></h3><p>个性化推荐是业界最常用的一种推荐产品形态，下面对常用个性化推荐算法来说明该算法所使用的数据集和相关特征工程方面的知识点。</p>
<h4 id="4-3-1-基于内容的个性化推荐"><a href="#4-3-1-基于内容的个性化推荐" class="headerlink" title="4.3.1 基于内容的个性化推荐"></a><strong>4.3.1 基于内容的个性化推荐</strong></h4><p>如果我们基于内容算法获得了每个标的物最相似的标的物列表，可以基于用户的操作历史记录，采用类似基于user-based或者item-based这类线性加权的方式获得用户的推荐，或者根据用户的操作历史获得用户的嵌入向量表示(用户操作过的标的物的向量表示的加权平均可以作为用户的嵌入表示)，通过计算用户向量和标的物<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E5%90%91%E9%87%8F%E5%86%85%E7%A7%AF">向量内积</a>来获得用户对标的物的评分，根据评分降序排序获得最终给用户的推荐列表。这类算法只需要用户操作行为数据及标的物相关数据，并不需要复杂的特征工程。</p>
<h4 id="4-3-2-基于协同过滤的个性化推荐"><a href="#4-3-2-基于协同过滤的个性化推荐" class="headerlink" title="4.3.2 基于协同过滤的个性化推荐"></a><strong>4.3.2 基于协同过滤的个性化推荐</strong></h4><p>常规的user-based、item-based、矩阵分解协同过滤，只需要用户行为数据，经过简单的算法就可以获得给用户的推荐。</p>
<h4 id="4-3-3-基于多数据源构建复杂特征的召回、排序模型"><a href="#4-3-3-基于多数据源构建复杂特征的召回、排序模型" class="headerlink" title="4.3.3 基于多数据源构建复杂特征的召回、排序模型"></a><strong>4.3.3 基于多数据源构建复杂特征的召回、排序模型</strong></h4><p>上面我们提到的算法都是基于简单规则或者人工假设的模型，基本不涉及到复杂的特征工程，我们都是一笔带过。下面我们提到的推荐召回、排序模型，都是需要通过构建复杂的特征来训练的，这类算法有多种，比如经典的<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=logistic%E5%9B%9E%E5%BD%92">logistic回归</a>、FM算法，还有最近比较火的深度学习等。这里我们重点讲解视频场景中，用经典的logistic回归模型来做推荐排序中涉及到的特征工程相关的知识点，其他模型类似。</p>
<p>对于利用logistic模型来预测用户对某个视频的点击率，样本是用户观看过的视频，每个用户视频对构成一个样本。这里正样本是用户点击播放过的视频(或者是播放时长超过一定时间的视频)，而负样本可以是曝光给用户但是用户没有点击过的视频(或者播放时间少于某个阈值的视频)。</p>
<p>这类构建复杂特征来训练召回模型的方法，需要尽可能多的数据，才能构建足够多样的特征，所以一般用户行为数据、用户特征数据、标的物特征数据、上下文数据等都会被使用到。下面分别对这4类数据构建的特征加以简单介绍。</p>
<ol>
<li><p>行为特征：视频的播放时长、播放完整度、点赞、转发、分享、评论等多种互动行为都与用户的最终点击有关，都是可以构建成特征的。</p>
</li>
<li><p>用户属性特征：是否是会员，地域，活跃度，登陆时间分布，观看节目分布，平均观看时长，搜索行为，点赞行为，收藏行为，是否高危地区，设备信息，当前版本。</p>
</li>
<li><p>节目属性特征：是否是会员节目，是否获奖，豆瓣得分，年代，是否高清(isHd)，地区，节目语言(language)，上映时间(show_time)等。</p>
</li>
<li><p>场景特征：是否节假日、用户所在路径，以及上下文(context)特征，context 特征通常是客户端带的信息，在用户授权的前提下可以直接获取，比如请求时间、用户手机品牌、手机型号、操作系统、当前网络状态（3g&#x2F;4g&#x2F;wifi）、用户渠道等实时属性特征以及之间的交叉特征。</p>
</li>
</ol>
<p>基于上面这些信息，我们可以构建4大类特征(基于第三节3中特征工程相关知识点)，形成如下面图8的模型训练样本集合，通过特征构建、特征选择、训练、离线评估等步骤我们最终获得了一个训练好的logistic回归模型。</p>
<img src="/2024/08/24/text1/1724476157035.png" class="" width="1724476157035">

<p>通过上面的介绍我们知道经典的个性化推荐算法，如item-based、user-based、矩阵分解、KNN等属于基于简单规则(假设)的模型，模型是我们人为事先设定好的，比如利用内积来表示相似度，某个样本点最近的K个点的类别进行投票确定该点的类别，这类模型没有需要学习的参数(KNN和矩阵分解中的隐因子数量都是人为事先确定的，属于超参数)，这类算法只需要进行简单的数据处理或者训练就可以为用户生成推荐，而包含大量参数的模型(如线性模型、FM、深度学习模型等)是需要进行复杂的特征工程的。</p>
<h2 id="5-未来发展和趋势"><a href="#5-未来发展和趋势" class="headerlink" title="5.未来发展和趋势"></a>5.未来发展和趋势</h2><p>伴随科技的发展，互联网技术渗透到了生活的方方面面，各类互联网产品层出不穷，而产品所提供的信息和服务更是爆炸性增长，这其中非常重要的一个帮助用户筛选、过滤信息的有效工具就是推荐系统，推荐系统一定是未来产品的标配技术，它一定会在更多的场景中得到应用。随着深度学习、强化学习等更多更新的技术应用于推荐系统，推荐的效果越来越好。随着各类传感器种类的丰富、交互方式的多样化，我们可以收集到越来越多样的数据。用户对反馈及时性的强烈需要，推动互联网产品更加实时地处理用户的请求，为用户提供更加实时化的交互体验。从这些大的趋势来看，推荐系统依赖的数据和特征工程处理技术也会面临更多的变化与挑战，下面作者就基于自己的理解和判断来说说在未来几年推荐系统数据与特征工程的几个重大变化和发展趋势。</p>
<p><strong>5.1 融合更多的数据源来构建更复杂的推荐模型</strong></p>
<p>数据是一切机器学习的基础，只有数据量够多、质量够好，才可以构建高质量的推荐模型。我们在这篇文章中总结了推荐系统四种数据源：行为数据、用户属性数据、物品属性数据、上下文数据。每类数据的种类趋向于多样化，拿行为数据来说，我们可以收集用户在手机屏上的滑动幅度、滑动快慢、按压力度等数据，在车载系统或者智能音箱场景中可以收集用户与机器的语音交互数据，在VR&#x2F;AR场景中我们可以收集用户手势、眼动、头部转动等各种信息，如果智能推荐未来拓展到这些场景中，那么无疑这些数据对构建推荐系统是非常关键的。在社交场景中，用户的社交关系数据对构建推荐系统也是非常重要的。如果某个公司有多个APP，那么用户在多个APP上的行为可以联合起来进行学习，构建更好的推荐系统。</p>
<p><strong>5.2 深度学习等复杂技术减少人工特征工程的投入</strong></p>
<p>深度学习技术在很多应用场景下构建的模型效果比传统算法好很多，深度学习另外一个被大家称道的优点是可以减少人工特征工程的繁琐投入，直接将较原始的数据灌入模型，让深度模型自动学习输入与输出之间的复杂非线性关系。</p>
<p>随着深度学习技术在推荐系统中的应用逐步成熟，目前应用到了非常多的行业及场景中，获得了非常好的效果。未来的趋势是基于特定应用场景，基于多样化数据构建场景化的特殊深度网络结构来获得更好的推荐效果。有了深度学习的加持，可以让推荐算法工程师将更多的时间用于思考业务、构建更加有效的模型上，而不是将大量精力放到构建复杂的特征上，从而解放推荐算法工程师的双手。</p>
<p><strong>5.3 实时数据处理与实时特征工程</strong></p>
<p>随着社会的发展，人们的生活节奏加快(大城市尤其如此，中小城市也会朝这个趋势发展)，人们的时间会更加碎片化，怎样更好地占领用户的碎片化时间，是任何一个公司在开发产品过程中需要关注的重要问题。这种社会趋势的出现，导致了新闻资讯、短视频应用的流行，在这些应用中用户消费一个标的物的时间是非常短的(一般几秒钟到几分钟)，在这类产品上构建推荐服务这就要求推荐系统可以近实时响应用户的需求，因此我们需要近实时收集处理数据、近实时构建特征工程、近实时训练算法模型、近实时响应用户需求变化。这一趋势一定是未来非常重要且非常难的一个方向。</p>
<p><strong>5.4 自动化特征工程</strong></p>
<p>自动化机器学习(AutoML，全称是Automated Machine Learning，参考文献4是一篇关于自动机器学习最近进展的<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E7%BB%BC%E8%BF%B0%E6%96%87%E7%AB%A0">综述文章</a>，参考文献5是一篇关于利用深度学习技术做自动化特性的文章，参考文献14是一篇关于自动化进行特征交叉的文章，读者可以自行学习)，是2014年以来，机器学习和深度学习领域最炙手可热的研究方向之一。机器学习过程需要大量的人工干预，这些人工干预表现在：特征提取、模型选择、参数调节等机器学习的各个方面。AutoML试图将这些与特征、模型、优化、评价有关的重要步骤自动化，使得机器学习模型无需人工干预即可自动化地学习与训练。</p>
<p>自动化特征工程也是AutoML研究中非常重要的一环，在AutoML中，自动特征工程的目的是自动地发掘并构造相关的特征，使得模型有更好的表现。除此之外，还包含一些特定的特征增强方法，例如特征选择、特征降维、特征生成、以及特征编码等。这些步骤目前来说都没有达到自动化的程度。</p>
<p>自动化特征工程目前虽然是学术研究中非常火的一个方向，但目前还不够成熟，也没有非常有效的方法很好地应用于各类业务场景中。自动特征工程在真实业务场景中确实是一个比较强的需求，因为手动构建特征工程需要很强的领域知识，需要懂机器学习的领域专家参与，而既懂机器学习又对业务理解透彻的人毕竟是少数。对于提供机器学习服务的云计算公司或者toB创业公司，需要接触到很多不同的行业，也不可能为每个领域配备一个专家，因此也希望可以通过技术手段来做自动特征工程，以减少人力和资源的投入，加快项目的推进进度，在这方面国内的明星创业公司<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E7%AC%AC%E5%9B%9B%E8%8C%83%E5%BC%8F">第四范式</a>做了很多尝试。</p>
<p>对于自动特征工程在推荐系统中的应用，读者可以看看参考文献12、13，文章中提供了很多很好的方法和思路。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a><strong>参考文献</strong></h2><p>1.图书：《精通特征工程》</p>
<p>2.[2.2w字长文详解推荐系统之数据与特征工程] <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/100864696">https://zhuanlan.zhihu.com/p/100864696</a></p>
<p><strong>3.[特征工程到底是什么？] <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/29316149">https://www.zhihu.com/question/29316149</a></strong></p>
<p>4.[2019] Taking the Human out of Learning Applications- A Survey on Automated Machine Learning</p>
<p>5.[2019] AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks</p>
<p>6.[2016 YouTube] Deep Neural Networks for YouTube Recommendations</p>
<p>7.[2016 Google] Wide &amp; Deep Learning for Recommender Systems</p>
<p>8.[浅谈微视推荐系统中的特征工程] <a href="https://link.zhihu.com/?target=https://mp.weixin.qq.com/s?__biz=MjM5ODYwMjI2MA==&mid=2649744942&idx=1&sn=7efd84c1371d785d719de481e3e6d44a&scene=21%23wechat_redirect">https://mp.weixin.qq.com/s/EgiSIJCRfiRLKwHUC1m46A</a></p>
<p>9.[如何解决特征工程，克服工业界应用 AI 的巨大难关] <a href="https://link.zhihu.com/?target=https://www.infoq.cn/article/solve-feature-engineering-in-industry/%23">https://www.infoq.cn/article/solve-feature-engineering-in-industry/#</a></p>
<p>10.<a href="https://link.zhihu.com/?target=http://spark.apache.org/docs/latest/ml-features.html">http://spark.apache.org/docs/latest/ml-features.html</a></p>
<p>11.<a href="https://link.zhihu.com/?target=https://scikit-learn.org/stable/data_transforms.html">https://scikit-learn.org/stable/data_transforms.html</a></p>
<p>12.[AutoML 在推荐系统中的应用] <a href="https://link.zhihu.com/?target=https://gitbook.cn/books/5bcd96da48da2b3b6ac43327/index.html">https://gitbook.cn/books/5bcd96da48da2b3b6ac43327/index.html</a></p>
<p>13.[罗远飞：自动特征工程在推荐系统中的研究] <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/98006452">https://zhuanlan.zhihu.com/p/98006452</a></p>
<p>14.[2019] AutoCross: Automatic Feature Crossing for Tabular</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="http://example.com">Chiyomi</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="http://example.com/2024/08/24/text1/">http://example.com/2024/08/24/text1/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/">特征工程</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="prev-post pull-full" href="/2024/08/25/D2Lnotes1/" title="D2L笔记-微积分部分"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Previous</div><div class="prev_info">D2L笔记-微积分部分</div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info is-center"><div class="avatar-img"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Chiyomi</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">9</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">3</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E7%89%B9%E5%BE%81%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">1.</span> <span class="toc-text">1.特征预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1%E7%BC%BA%E5%A4%B1%E5%80%BC%E5%A4%84%E7%90%86"><span class="toc-number">1.1.</span> <span class="toc-text">1.1缺失值处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2%E5%BD%92%E4%B8%80%E5%8C%96"><span class="toc-number">1.2.</span> <span class="toc-text">1.2归一化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3%E5%BC%82%E5%B8%B8%E5%80%BC%E4%B8%8E%E6%95%B0%E5%80%BC%E6%88%AA%E6%96%AD"><span class="toc-number">1.3.</span> <span class="toc-text">1.3异常值与数值截断</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2%EF%BC%9A"><span class="toc-number">1.4.</span> <span class="toc-text">1.4非线性变换：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E7%89%B9%E5%BE%81%E6%9E%84%E5%BB%BA"><span class="toc-number">2.</span> <span class="toc-text">2.特征构建</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1%E7%A6%BB%E6%95%A3%E7%89%B9%E5%BE%81"><span class="toc-number">2.1.</span> <span class="toc-text">2.1离散特征</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2%E8%BF%9E%E7%BB%AD%EF%BC%88%E6%95%B0%E5%80%BC%EF%BC%89%E7%89%B9%E5%BE%81"><span class="toc-number">2.2.</span> <span class="toc-text">2.2连续（数值）特征</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3%E6%97%B6%E7%A9%BA%E7%89%B9%E5%BE%81"><span class="toc-number">2.3.</span> <span class="toc-text">2.3时空特征</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4%E6%96%87%E6%9C%AC%E7%89%B9%E5%BE%81"><span class="toc-number">2.4.</span> <span class="toc-text">2.4文本特征</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5%E5%AF%8C%E5%AA%92%E4%BD%93%E7%89%B9%E5%BE%81"><span class="toc-number">2.5.</span> <span class="toc-text">2.5富媒体特征</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-6%E5%B5%8C%E5%85%A5%E7%89%B9%E5%BE%81"><span class="toc-number">2.6.</span> <span class="toc-text">2.6嵌入特征</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9"><span class="toc-number">3.</span> <span class="toc-text">3.特征选择</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1%E5%9F%BA%E4%BA%8E%E7%BB%9F%E8%AE%A1%E9%87%8F%E9%80%89%E6%8B%A9"><span class="toc-number">3.1.</span> <span class="toc-text">3.1基于统计量选择</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E5%B8%B8%E7%94%A8%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95"><span class="toc-number">4.</span> <span class="toc-text">4.常用推荐算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E6%8E%92%E8%A1%8C%E6%A6%9C%E6%8E%A8%E8%8D%90"><span class="toc-number">4.1.</span> <span class="toc-text">4.1 排行榜推荐</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E6%A0%87%E7%9A%84%E7%89%A9%E5%85%B3%E8%81%94%E6%A0%87%E7%9A%84%E7%89%A9%E6%8E%A8%E8%8D%90"><span class="toc-number">4.2.</span> <span class="toc-text">4.2 标的物关联标的物推荐</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-1-%E5%9F%BA%E4%BA%8E%E5%86%85%E5%AE%B9%E7%9A%84%E5%85%B3%E8%81%94%E6%8E%A8%E8%8D%90"><span class="toc-number">4.2.1.</span> <span class="toc-text">4.2.1 基于内容的关联推荐</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-2-%E5%9F%BA%E4%BA%8E%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%9A%84%E5%85%B3%E8%81%94%E6%8E%A8%E8%8D%90"><span class="toc-number">4.2.2.</span> <span class="toc-text">4.2.2 基于协同过滤的关联推荐</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90"><span class="toc-number">4.3.</span> <span class="toc-text">4.3 个性化推荐</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-1-%E5%9F%BA%E4%BA%8E%E5%86%85%E5%AE%B9%E7%9A%84%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90"><span class="toc-number">4.3.1.</span> <span class="toc-text">4.3.1 基于内容的个性化推荐</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-2-%E5%9F%BA%E4%BA%8E%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%9A%84%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90"><span class="toc-number">4.3.2.</span> <span class="toc-text">4.3.2 基于协同过滤的个性化推荐</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-3-%E5%9F%BA%E4%BA%8E%E5%A4%9A%E6%95%B0%E6%8D%AE%E6%BA%90%E6%9E%84%E5%BB%BA%E5%A4%8D%E6%9D%82%E7%89%B9%E5%BE%81%E7%9A%84%E5%8F%AC%E5%9B%9E%E3%80%81%E6%8E%92%E5%BA%8F%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.3.3.</span> <span class="toc-text">4.3.3 基于多数据源构建复杂特征的召回、排序模型</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E6%9C%AA%E6%9D%A5%E5%8F%91%E5%B1%95%E5%92%8C%E8%B6%8B%E5%8A%BF"><span class="toc-number">5.</span> <span class="toc-text">5.未来发展和趋势</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="toc-number">6.</span> <span class="toc-text">参考文献</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/10/09/AKT-note/" title="AKT-note">AKT-note</a><time datetime="2024-10-09T06:59:28.000Z" title="Created 2024-10-09 14:59:28">2024-10-09</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/10/08/note10-8/" title="note10-8">note10-8</a><time datetime="2024-10-08T06:55:50.000Z" title="Created 2024-10-08 14:55:50">2024-10-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/10/04/SAKT-notes/" title="SAKT-note">SAKT-note</a><time datetime="2024-10-04T01:17:18.000Z" title="Created 2024-10-04 09:17:18">2024-10-04</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/09/20/KT-notes920/" title="论文阅读笔记920">论文阅读笔记920</a><time datetime="2024-09-20T06:45:32.000Z" title="Created 2024-09-20 14:45:32">2024-09-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/09/13/KT-summarize2024/" title="KT-summarize2024">KT-summarize2024</a><time datetime="2024-09-13T12:57:31.000Z" title="Created 2024-09-13 20:57:31">2024-09-13</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By Chiyomi</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>