<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>AKT-note | Memoryβ</title><meta name="author" content="Chiyomi"><meta name="copyright" content="Chiyomi"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="论文标题：Context-Aware Attentive Knowledge Tracing 作者与发表日期： Ghosh, Aritra and Heffernan, Neil and Lan, Andrew S.  期刊或会议名称：KDD ‘20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining Vi">
<meta property="og:type" content="article">
<meta property="og:title" content="AKT-note">
<meta property="og:url" content="http://example.com/2024/10/09/AKT-note/index.html">
<meta property="og:site_name" content="Memoryβ">
<meta property="og:description" content="论文标题：Context-Aware Attentive Knowledge Tracing 作者与发表日期： Ghosh, Aritra and Heffernan, Neil and Lan, Andrew S.  期刊或会议名称：KDD ‘20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining Vi">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/img/butterfly-icon.png">
<meta property="article:published_time" content="2024-10-09T06:59:28.000Z">
<meta property="article:modified_time" content="2024-10-10T08:14:02.942Z">
<meta property="article:author" content="Chiyomi">
<meta property="article:tag" content="readingnotes">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/butterfly-icon.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2024/10/09/AKT-note/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        if (name && globalFn[key][name]) return
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'AKT-note',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-10-10 16:14:02'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Memoryβ</span></a><a class="nav-page-title" href="/"><span class="site-name">AKT-note</span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">AKT-note</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-10-09T06:59:28.000Z" title="Created 2024-10-09 14:59:28">2024-10-09</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2024-10-10T08:14:02.942Z" title="Updated 2024-10-10 16:14:02">2024-10-10</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><ul>
<li><strong>论文标题</strong>：Context-Aware Attentive Knowledge Tracing</li>
<li><strong>作者与发表日期</strong>： Ghosh, Aritra and Heffernan, Neil and Lan, Andrew S. </li>
<li><strong>期刊或会议名称</strong>：KDD ‘20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining Virtual Event CA USA July 6 - 10, 2020 </li>
<li><strong>链接</strong>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2007.12324">https://arxiv.org/abs/2007.12324</a></li>
<li><strong>DOI</strong>： 10.1145&#x2F;3394486.3403282 </li>
<li><strong>摘要：</strong> 知识追踪 （KT） 是指根据学习者过去在教育应用程序中的表现来预测未来学习者的表现的问题。使用基于灵活深度神经网络的模型的 KT 的最新发展在这项任务中表现出色。然而，这些模型的可解释性通常有限，因此不足以进行个性化学习，这需要使用可解释的反馈和可操作的建议来帮助学习者获得更好的学习成果。在本文中，我们提出了注意力知识追踪 （AKT），它将灵活的基于注意力的神经网络模型与一系列受认知和心理测量模型启发的新型、可解释的模型组件相结合。AKT 使用一种新颖的单调注意力机制，将学习者未来对评估问题的回答与他们过去的回答联系起来;除了问题之间的相似性外，注意力权重是使用指数衰减和上下文感知相对距离测量来计算的。此外，我们使用 Rasch 模型来正则化概念和问题嵌入;这些嵌入能够捕获关于同一概念的问题之间的个体差异，而无需使用过多的参数。我们在几个真实世界的基准数据集上进行了实验，结果表明 AKT 在预测未来学习者的反应方面优于现有的 KT 方法（在某些情况下，AUC 高达 6%）。我们还进行了几个案例研究，表明 AKT 表现出出色的可解释性，因此在现实世界的教育环境中具有自动反馈和个性化的潜力。</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><img src="/2024/10/09/AKT-note/1728459097024.png"  alt="1728459097024" style="zoom:50%;" />

<p>ht代表t时刻的知识水平，rt代表t时刻对评估问题的回答评分（通常是二进制值，1代表回答正确，0则反）。因此前一个函数称为回答模型（response model），后一个称为知识进化模型（knowledge evolution model）。</p>
<p><strong>其他模型和方法：</strong></p>
<ul>
<li><strong>DKT：</strong>每个学习者对专家定义的概念的 Nowledge 级别。KT 的最新发展集中在：使用更复杂和灵活的模型来充分利用大规模学习者响应数据集中所包含的信息。深度知识追踪 （DKT） 方法是第一种通过使用长短期记忆 （LSTM） 网络作为知识进化模型 g（·） 来探索（可能是深度）神经网络用于 KT 的方法。由于 LSTM 单元是非线性的复杂函数，因此它们比仿射变换更灵活，并且更能够捕获真实数据中的细微差别</li>
<li><strong>DKVMN：</strong>动态键值记忆网络 （DKVMN） 方法通过使用外部记忆矩阵 （Ht） 来表征学习者知识 。该矩阵分为两部分：一个静态的“关键”矩阵，其中包含每个概念的固定表示，以及一个动态的“价值”矩阵，其中包含每个学习者对每个概念的不断发展的知识水平。DKVMN 还在这个外部矩阵上为响应和知识进化模型使用单独的 “read” 和 “write” 过程;这些过程使其比 DKT 更灵活。DKT 和 KVMN 报告了预测未来学习者表现的最新性能 ，并已成为新 KT 方法的基准。</li>
<li><strong>SAKT：</strong>自我关注知识追踪（SAKT）方法是在KT环境中使用注意力机制的第一种方法。注意力机制比递归和基于记忆的神经网络更灵活，并且在自然语言处理任务中表现出卓越的性能。SAKT 的基本设置与 Transformer 模型 有许多相似之处，Transformer 模型是许多序列到序列预测任务的有效模型。然而，我们观察到 SAKT 在我们的实验中并没有优于 DKT 和 DKVMN 。可能的原因包括：<br>   <strong>i）</strong> <strong>与单词之间常常具有强烈长距离依赖性的语言任务不同，未来学习者表现对过去的依赖性可能仅限于更短的窗口</strong>；<br>   <strong>ii） 学习者响应数据集的大小比自然语言数据集小几个数量级，并且不太可能从高度灵活和大规模的注意力模型中受益。</strong></li>
</ul>
<p>更重要的是，没有现有的 KT 方法真正擅长<strong>未来性能预测</strong>和<strong>可解释性</strong>。早期的 KT 方法表现出出色的可解释性，但在预测未来学习者的表现方面没有提供最先进的性能。最近基于深度学习的 KT 方法在这方面表现出色，但提供的可解释性有限。<strong>因此，</strong>这些知识转移方法并不能完全满足个性化学习的需求，这不仅需要准确的表现预测，还需要提供自动化、可解释的反馈和可操作的建议的能力，以帮助学习者获得更好的学习成果。</p>
<h3 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h3><p><em>为了预测学习者对当前问题的回答，我们提出了注意力知识追踪 （AKT） 方法，该方法使用一系列注意力网络在这个问题和学习者过去回答的每个问题之间建立联系。我们总结了以下主要创新：</em></p>
<ul>
<li>与使用原始问题和响应嵌入的现有注意力方法相反，我们<strong>将原始嵌入放入上下文中</strong>，并通过考虑学习者的整个实践历史，<strong>使用过去问题和响应的上下文感知表示</strong>。</li>
<li>受认知科学关于遗忘机制的发现的启发，我们提出了一种<strong>新的单调注意力机制</strong>，它使用<strong>指数衰减曲线</strong>来降低遥远过去问题的重要性。我们还开发了一个上<strong>下文感知度量</strong>来描述学习者过去回答的问题之间的时间间隔。</li>
<li>利用 <strong>Rasch 模型</strong>，一种简单且可解释的 IRT 模型，我们使用一系列基于 Rasch 模型的嵌入来捕捉问题之间的个体差异，而无需引入过多的模型参数。</li>
</ul>
<p><em>我们在几个基准真实世界的教育数据集上进行了一系列实验，将 AKT 与最先进的 KT 方法进行比较。我们的结果表明，AKT（有时显着）在预测未来学习者的表现方面优于其他 KT 方法。此外，我们对每个关键的 AKT 模型组件进行了消融研究，以证明它们的价值。我们还进行了几个案例研究，以表明 AKT 表现出<strong>出色的可解释性</strong>，并且具有<strong>自动反馈和练习题推荐的潜力</strong>，这两者都是个性化学习的关键要求。</em></p>
<hr>
<h3 id="Problem-Setup"><a href="#Problem-Setup" class="headerlink" title="Problem Setup"></a>Problem Setup</h3><p>每个学习者的表现记录都包含每个离散时间步长的一系列问题和回答。对于时间步 t 的学习者 i，我们表示他们回答的问题、这个问题涵盖的概念以及他们以元组形式给出的分级响应的组合<em><strong>（qit ， cti ， rti ）</strong></em>，其中 qit ∈ N+ 是<strong>问题索引</strong>，cti ∈ N+ 是<strong>概念索引</strong>，rti ∈ {0， 1} 是回答。在此表示法下，（qit ， cti ， 1） 表示学习者 i 在时间 t 正确回答了概念 cti 上的问题 qit。此设置与之前的一些深度知识跟踪工作不同，这些工作经常忽略问题索引并将学习者的表现总结为 （cti ， rti ）。做出这个选择是为了避免过度参数化；在以下讨论中，我们省略了上标 i，因为我们讨论了如何预测<strong>单个学习者</strong>的未来表现。鉴于他们过去到时间 t − 1 的历史为 {（q1， c1， r1）， . . ， （qt −1， ct −1， rt −1）}，我们的目标是预测他们在当前时间步 t 对概念 ct 上的问题 qt 的回答 rt。</p>
<hr>
<p><strong>问题和回答的嵌入表达</strong></p>
<p>我们使用实值嵌入向量xt ∈ RD和yt ∈ RD来分别表示<strong>每个问题</strong>和<strong>每个问题-响应对</strong>（qt、rt）。</p>
<ul>
<li><strong>xt</strong> 描述<strong>有关问题的信息</strong> ;</li>
<li>而 <strong>yt</strong> 描述<strong>学习者通过回答问题获得的知识</strong>，分别使用<strong>两个单独的嵌入来表示正确和错误的回答</strong>。</li>
</ul>
<p>D 表示这些嵌入的维度。因此，让 Q 表示问题的数量，总共有 Q 个问题嵌入向量和 2Q 个问题-响应嵌入向量。在大多数现实世界的教育环境中，题库比概念集大得多，并且许多问题被分配给极少数学习者。因此，大多数现有的 KT 方法都<strong>使用概念来索引问题以避免过度参数化</strong> ; <em>涵盖同一概念的所有问题都被视为单个问题</em>。在这种情况下，qt &#x3D; ct 即Q &#x3D; C。</p>
<hr>
<p><strong>AKT 方法由四个部分组成：</strong></p>
<p>1）两个自注意<strong>编码器</strong>，一个用于问题，一个用于知识获取，</p>
<p>2）一个基于注意力的<strong>知识检索器</strong></p>
<p>3）一个<strong>前馈响应预测模型</strong></p>
<p>下图直观地显示了 AKT 方法及其连接组件。</p>
<img src="/2024/10/09/AKT-note/1728463295714.png" class="" width="1728463295714">

<blockquote>
<p><em>图 1：AKT 方法概述。我们使用基于 Rasch 模型的嵌入作为问题和响应的原始嵌入。问题和知识编码器计算问题和响应对的上下文感知表示形式。知识检索器使用这些表示形式作为输入，并计算学习者的知识状态。为简单起见，我们不在编码器中展示单调注意力机制。我们也不显示子图层。</em></p>
</blockquote>
<p>我们使用两个自我关注编码器来学习问题和回答的上下文感知表示。我们将第一个编码器称为<strong>问题编码器</strong>，它会根据学习者之前练习过的问题序列，为<strong>每个问题</strong>生成经过改良的、情境化的表示。同样，我们将第二个编码器称为<strong>知识编码器</strong>，它生成学习者在回答过去问题时<strong>获得的知识</strong>的改良后的、情境化表示。或者，我们可以使用类似于之前工作的问题和响应的原始嵌入。我们发现，<strong>上下文感知表示</strong>在大多数数据集中表现更好。我们将<strong>知识进化模型</strong>称为<strong>知识检索器</strong>，它使用注意力机制检索过去获得的<strong>与当前问题相关的知识。</strong>最后，<strong>响应预测模型</strong>使用检索到的知识<strong>预测</strong>学习者对当前问题的响应。</p>
<blockquote>
<p><em>AKT 方法的动机是植根于认知科学和心理测量学的三种直觉。</em></p>
</blockquote>
<hr>
<p><strong>上下文感知表达和知识检索器</strong></p>
<p>我们在模型中使用了两个编码器。问题编码器将<strong>原始问题嵌入</strong> {x1， . . . ， xt } 作为输入，并使用单调注意力机制输出一系列<strong>上下文感知问题嵌入</strong> {xˆ1， . . .， ˆxt }。每个问题的上下文感知嵌入<em><strong>取决于自身和过去</strong></em>的问题，即 ˆxt &#x3D; fenc1 （x1， . . . ， xt ）。</p>
<img src="/2024/10/09/AKT-note/1728469204291.png"  alt="1728469204291" style="zoom:67%;" />

<p>同样，知识编码器将<strong>原始问题-响应嵌入</strong> {y1， . . . ， yt−1} 作为输入，并使用相同的单调注意力机制输出获得的<strong>实际知识序列</strong> {yˆ1， . . . ， yˆt−1}。所获知识的语境感知嵌入<em><strong>取决于学习者对当前问题和过去问题的回答</strong></em>，即 yˆt−1 &#x3D; fenc2 （y1， . . . ， yt −1）。</p>
<img src="/2024/10/09/AKT-note/1728469216072.png"  alt="1728469216072" style="zoom:67%;" />

<blockquote>
<p><em>选择使用上下文感知嵌入而不是原始嵌入反映了我们的第一种直觉：学习者在回答问题时理解和学习的方式取决于学习者。这些修改后的表示反映了每个学习者对问题的实际理解以及他们根据个人回答历史实际获得的知识。这种模型选择的动机是出于直觉，即对于过去反应序列不同的两个学习者，他们理解同一问题的方式和他们从练习中获得的知识可能会有所不同。</em></p>
</blockquote>
<p>知识检索器将<strong>上下文感知</strong>问题<strong>和问题-响应对嵌入</strong> ˆx1：t 和 yˆ1：t −1 作为输入，并输出针对当前问题的检索到的<strong>知识状态 ht</strong>。我们注意到，在 AKT 中，学习者的当前知识状态<strong>也是上下文感知</strong>的，因为它取决于他们正在回答的当前问题 。知识检索器只能使用有关过去问题的信息、学习者对这些问题的回答以及当前问题的表示形式，而不能使用学习者对当前问题的回答，即 ht &#x3D; fkr（ˆx1， . . . ，xˆt ， yˆ1， . . . ，yˆt−1）。响应预测模型使用检索到的知识来预测当前响应。</p>
<hr>
<p><strong>单调注意力机制</strong></p>
<p>我们对编码器和知识检索器使用<strong>缩放点积注意力机制修改后的单调版本</strong>。我们首先简要总结一下最初的缩放点积注意力机制：在此框架下，每个编码器和知识检索器都有一个<strong>键</strong>、<strong>查询</strong>和<strong>值嵌入层</strong>，该层将输入分别映射到维度 Dq &#x3D; Dk 、 Dk 和 Dv 的输出查询、键和值。设 <strong>qt</strong> ∈ RDk ×1 表示与学习者在<strong>时间 t 回答的问题</strong>相对应的查询，<strong>缩放的点积注意力值</strong> <strong>αt，τ</strong> 使用 softmax 函数计算为</p>
<img src="/2024/10/09/AKT-note/1728470728912.png"  alt="1728470728912" style="zoom:67%;" />

<p>两个<strong>编码器都采用自注意力机制</strong>，即 qt 、 kt 和 vt（查询、键、值） 是使用<strong>相同的输入</strong>计算的  ; 问题编码器使用 {x1， . . . ， xt }，而知识编码器使用 {y1， . . . ， yt−1}。</p>
<p>另一方面，<strong>知识检索器不使用自我注意</strong>。在时间步 t 处，它使用：</p>
<ol>
<li>ˆxt<strong>（当前问题的修改嵌入）</strong>、{ ˆx1， . . . ， ˆxt−1}（<strong>过去问题的上下文感知嵌入）</strong></li>
<li>{yˆ1， . . . ， yˆt−1}（<strong>过去问题-响应对的上下文感知嵌入）</strong>作为输入</li>
</ol>
<p>来分别生成查询、键和值。我们注意到，SAKT 使用问题嵌入向量来映射查询，而使用响应嵌入向量来映射键和值。在我们的实验中，我们发现使用问题嵌入来映射查询和键要有效得多。</p>
<hr>
<p>然而，这种基本的缩放点积注意力机制对 KT 来说可能还不够。原因是学习是暂时的，记忆会衰减 ; 当我们预测学习者对当前问题的回答时，学习者在遥远过去的表现<strong>不如</strong>最近的表现提供信息。因此，我们开发了<strong>一种新的单调注意力机制</strong>来反映我们的第二直觉：当学习者面临一个新问题时，<strong>过去在不相关概念上的经验</strong> 和 <strong>太久以前的经验</strong>不太可能高度相关。具体来说，我们在注意力分数中添加一个<em><strong>乘法指数衰减项</strong></em>，如下所示：</p>
<img src="/2024/10/09/AKT-note/1728477973655.png"  alt="1728477973655" style="zoom:67%;" />

<p>以及：</p>
<img src="/2024/10/09/AKT-note/1728547248044.png"  alt="1728547248044" style="zoom:67%;" />

<p>其中 θ &gt; 0 是可学习的<strong>衰减率参数</strong>，<strong>d（t，τ ）</strong>是时间步长 t 和 τ 之间的<strong>时间距离测量</strong>。换句话说，当前问题对过去问题的<strong>关注权重不仅取决于对应 query 和 key 之间的相似度</strong>，<strong>还取决于它们之间的相对时间步长数</strong>。总之，我们的单调注意力机制采用随时间呈指数衰减曲线的基本形式，当过去的问题与当前问题高度相似时，时间步长可能会出现峰值。注意，我们将指数衰减应用于注意力权重而不是潜在知识，这是现有学习者模型中的常见方法。</p>
<hr>
<p><strong>上下文感知距离度量</strong></p>
<p>指数衰减函数决定了注意力权重随着当前时间索引与先前时间索引之间距离的增加而衰减的速率。定义两个时间索引之间距离的一种直接方法是它们的<strong>绝对值差</strong>。但是，此距离不是上下文感知的，并且会忽略每个学习者的练习历史记录。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="http://example.com">Chiyomi</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="http://example.com/2024/10/09/AKT-note/">http://example.com/2024/10/09/AKT-note/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/readingnotes/">readingnotes</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="next-post pull-full" href="/2024/10/08/note10-8/" title="note10-8"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Next</div><div class="next_info">note10-8</div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><a href="/2024/09/20/KT-notes920/" title="论文阅读笔记920"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-20</div><div class="title">论文阅读笔记920</div></div></a><a href="/2024/09/13/KT-summarize2024/" title="KT-summarize2024"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-13</div><div class="title">KT-summarize2024</div></div></a><a href="/2024/10/04/SAKT-notes/" title="SAKT-note"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-10-04</div><div class="title">SAKT-note</div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info is-center"><div class="avatar-img"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Chiyomi</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">9</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">3</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Introduction"><span class="toc-number">1.</span> <span class="toc-text">Introduction</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Contributions"><span class="toc-number">1.1.</span> <span class="toc-text">Contributions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Problem-Setup"><span class="toc-number">1.2.</span> <span class="toc-text">Problem Setup</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/10/09/AKT-note/" title="AKT-note">AKT-note</a><time datetime="2024-10-09T06:59:28.000Z" title="Created 2024-10-09 14:59:28">2024-10-09</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/10/08/note10-8/" title="note10-8">note10-8</a><time datetime="2024-10-08T06:55:50.000Z" title="Created 2024-10-08 14:55:50">2024-10-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/10/04/SAKT-notes/" title="SAKT-note">SAKT-note</a><time datetime="2024-10-04T01:17:18.000Z" title="Created 2024-10-04 09:17:18">2024-10-04</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/09/20/KT-notes920/" title="论文阅读笔记920">论文阅读笔记920</a><time datetime="2024-09-20T06:45:32.000Z" title="Created 2024-09-20 14:45:32">2024-09-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/09/13/KT-summarize2024/" title="KT-summarize2024">KT-summarize2024</a><time datetime="2024-09-13T12:57:31.000Z" title="Created 2024-09-13 20:57:31">2024-09-13</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By Chiyomi</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>